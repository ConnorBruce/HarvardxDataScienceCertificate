---
title: "Data Science Certificate Course"
author: "Connor Bruce"
output:
  html_document: default
  word_document: default
---

---
```{r, echo = F, eval = F}
install.packages("tidyverse", "dslabs", "NHANES","ggthemes", "ggrepel","gridExtra", "dplyr","RColorBrewer","readxl","rvest","htmlwidgets")
library(tidyverse)
library(dslabs)
library(NHANES)
library(ggthemes)
library(ggrepel)
library(gridExtra)
library(dplyr)
library(RColorBrewer)
library(gtools)
library(readxl)
library(rvest)
library(htmlwidgets)
```



## Course 1: R Basics

### Section 1: R Basics, Functions, and Data Types

#### 1.1 Motivation and Getting Started

* This course will focus on US crime statistics as the Case Study.
* install.packages("___") to install packages (must be connected to internet)
* library(___) to load package into script after it has been installed (only need to install once)  

Here is an example of loading packages and using them to create a graph
```{r, error = F,warning =F, message = F}
library(tidyverse)
library(dslabs)

data(murders)

murders %>%
  ggplot(aes(population, total, label = abb, color =region)) +
  geom_label()
```  

---

#### 1.2 R Basics  

##### Objects
* Objects can be variables, or other, more complicated things, such as functions
* ls() outputs a list of objects. Alternatively, it can been in the Environment tab at the top right.

Objects example: Variables being used to solve quadratic formula

```{r}
a = 1
b = 1
c = -1

(-b + sqrt(b^2-4*a*c))/(2*a)
(-b - sqrt(b^2-4*a*c))/(2*a)
```

##### Functions

* To see the code behind a function, type the function without the parenthesis into the console
* Nested functions: Call a function to get an argument for another function. Example:
```{r}
log(exp(1))
```
* To get help with a function, can use the help() function using the function as the argument.
```{r}
#help(log)
```
* Or you can check the arguments:
```{r}
args(log)
```  

---

#### 1.3 Data Types

##### Data Frames

* class() can be used to determine the type of data.
* str() can show the structure of the data frame
* head() shows the first 6 rows
* Can use $ as a variable accessor. Will return a vector with the column of the variables

```{r}
class(murders)
str(murders)
head(murders)
pop = murders$population
length(pop)
```

##### Other Data Types

* Character Strings
```{r}
class(murders$state)
```

* Logical Variables (True/False)
```{r}
z = 3 == 2
z
class(z)
```

##### Factors

* Good for storing categorical data (more memory efficient) - numbers associated with a character string
* Recommended to avoid
* table() can be used to show how many are in each category

```{r}
levels(murders$region)
table(murders$region)
```  

---

### Section 2: Vectors, Sorting

#### 2.1 Vectors

* Create vectors by using the concatenate function, c()
* Can assign names to the numeric values in a vector
* Two ways to assign names are shown below

```{r}
codes = c(380,124,818)
country = c("italy", "canada","egypt")
names(codes) = country
codes2 = c(italy = 380, canada = 124, egypt = 818)
codes
codes2
```

* seq(a,b,c) provides a vector from a to b by jumps of c
* Can also do a sequence using :
```{r}
seq(1,10)
1:10
seq(1,100,10)
```

##### Subsetting

* [] are used to access elements of a vector
* Indexes at 1
* Below are examples of pulling different elements from the codes vector
```{r}
codes[2]
codes[c(1,3)]
codes["canada"]
codes[c("egypt","italy")]
```

##### Vector Coercion

* Coercion is an attempt by r to be flexible with data types.
* The below code should throw an error because all elements of a vector must be the same type, however R coerces 1 and 3 into character variables in order to avoid and error
```{r}
x = c(1,"canada",3)
x
```

* Variables can be converted into different types
```{r}
x = 1:5
x
y = as.character(x)
y
as.numeric(y)
```

* Can only coerce in certain ways and if impossible, will replace with NA
```{r}
x = c("1", "b", "2")
as.numeric(x)
```

---

#### 2.2 Sorting

* sort() sorting into ascending order
* order() provides the index of the values in ascending order
* Can use the index to extract the necessary information
```{r}
sort(murders$total)
order(murders$total)
index = order(murders$total)
murders$abb[index]
```

* max() gives the maximum value in a vector, which.max() give the index of that value

```{r}
max(murders$total)
which.max(murders$total)
i_max = which.max(murders$total)
murders$state[i_max]
```

* rank() gives the "rank" of each value in the vector (1 being the smallest value) (also works with min)

```{r}
x = c(31,4,15,92,65)
rank(x)
```

* Here is an overall comparison of sort(), order(), and rank()

```{r}
x = c(31,4,15,92,65)
sortx = sort(x)
orderx = order(x)
rankx = rank(x)
sorderank = matrix(c(x,sortx,orderx,rankx), ncol = 4)
colnames(sorderank) = c("original", "sort", "order", "rank")
rownames(sorderank) = c(" "," "," "," "," ")
sorderank
```

* Can use is.na() to find which entries in a vector are NA (returns as a vector of logical variables)
* True = 1, False = 0
* Can also use [!___] to take everything from a vector that is not something.

---

#### 2.3 Vector Arithmetic

* We can see that California has the most murders and can also see that California also has the largest population
```{r}
murders$state[which.max(murders$population)]
```

* Therefore, we should be looking at the murders per capita
* It is possible to scale a vector. e.g vector to inches to centimeters
```{r}
height = c(12,23,15,32,42,26,51,10)
height * 2.54
```
* Now we can do this with the murders vector to get a rate of murders per 100,000 and then order those to find which has the highest rate and the lowest rate. It can be seen that, while California has the most murders, it ranks 14th in terms of murder rate.
```{r}
murder_rate = murders$total/murders$population*100000
murders$state[order(murder_rate, decreasing = T)]
```

---

### Section 3: Indexing, Data Wrangling, Plots  

#### 3.1 Indexing

* We can subset a vector based on properties of another vector.
* We can use logical operators to index vectors.
* Suppose we want to figure out which states have a murder rate of <= .71 per 100,000:

```{r}
index = murder_rate <= .71
index
murders$state[index]
sum(index)
```

* Now supposed we want to move to a state in the Western region that has a murder rate that is <= 1.
```{r}
west = murders$region == "West"
safe = murder_rate <= 1
index = safe & west
murders$state[index]
```

##### Indexing Functions

* which() gives entries of a logical vector that are true
* match() looks for entries in a vector and returns the index needed to access them.
* %in% determines whether each element of a first vector is in a second vector.

```{r}
x = c(FALSE, TRUE, FALSE, TRUE, TRUE, FALSE)
which(x)
index = match(c("New York","Florida","Texas"), murders$state)
index
murders$state[index]
x = c("a","b","c","d","e")
y = c("a","d","f")
y %in% x
c("Boston", "Dakota", "Washington") %in% murders$state
```

---

#### 3.2: Data Wrangling

First, the dplyr package must be installed and loaded in
```{r}
library(dplyr)
```

dplyr Functions:  

* mutate() can be used to change a data table by adding a new column or changing an existing one. Works in the dataframe, not the workspace
* filter() can be used to filter the data by subsetting rows.
* select() can be used to subset data by selecting specific columns.
 
Using this, we can now edit our murders data frame to add the information we want to add:
```{r}
murders = mutate(murders, rate = total/population*100000)
head(murders)
filter(murders, rate <= .71)
new_table = select(murders, state, region, rate)
filter(new_table, rate <= .71)
```

In order to streamline, results can be sent from one function to another function using the pipe operator %>%:

```{r}
murders %>% select(state, region, rate) %>% filter(rate <= .71)
```

##### Data Frames

* Data frames can be created using the data.frame() function
* Columns are created using name = data, name = data, name = data, ...
* Warning: data.frame turns character variables into factors, to avoid, add stringsAsFactors = FALSE
Example:
```{r}
grades = data.frame(names = c("John", "Juan", "Jean","Yao"),exam_1 = c(95,80,90,85),exam_2 = c(90,85,85,90), stringsAsFactors = F)
grades
```

---

#### 3.3: Basic Plots
More will be learned on plots during the course based on the ggplot2 package.  

Below are 3 different types of plots: scatterplot, histogram, and boxplot. All are related to the murders data set that we have been working with:

```{r}
x = murders$population / 10^6
y = murders$total
plot(x,y)

hist(murders$rate)

boxplot(rate~region, data=murders)
```  

---

### Section 4: Programming Basics

#### 4.2 Conditionals

* Most common conditional is the if/else statement  

Let's use the if/else statement to see if any of the states have a murder rate lower that .5, and print them if they do:

```{r}
ind = which.min(murder_rate)
if(murder_rate[ind] < .5) {
  print(murders$state[ind])
} else {
  print("No state has murder rate that low")
}

if(murder_rate[ind] < .25) {
  print(murders$state[ind])
} else {
  print("No state has murder rate that low")
}
```

* Can also use ifelse(boolean statement, return if true, return if false) statement
* The ifelse statement can be useful because it can be used on vectors

```{r}
a=0
ifelse(a > 0,1/a, NA)
a=5
ifelse(a > 0,1/a, NA)
a = c(0,1,2,-4,5)
ifelse(a > 0,1/a, NA)
```

* Another example using na_example. Say we want to replace all the NAs in the data set with 0, we can use ifelse to do this:

```{r}
data(na_example)
sum(is.na(na_example))
no_nas = ifelse(is.na(na_example),0,na_example)
```
* any and all are useful function: any takes a vector and returns TRUE if there is at least one TRUE, all returns TRUE if all of the entries are TRUE:

```{r}
z = c(T,T,F)
z2 = c(F,F,F)
z3 = c(T,T,T)
any(z)
all(z)
any(z2)
all(z2)
any(z3)
all(z3)
```

---

#### 4.3 Functions

* Functions are useful for operations that need to be done multiple times.
* Values/variables in functions only live in the function, not in the global environment
* Format to create a function: my_function = function(VARIABLES){operation that operate on VARIABLES and return value}

Example for creating the mean function:
```{r}
avg = function(x){
  s = sum(x)
  n = length(x)
  s/n
}
avg(c(1,2,3,4,5,6,7))

x = 1:100
identical(mean(x),avg(x))
```

Function that computes either arithmetic or geometric average:
```{r}
avg2 = function(x, arithmetic = T){
  n = length(x)
  ifelse(arithmetic, sum(x)/n, prod(x)^(1/n))
}
avg2(1:10)
avg2(1:10, F)
```

---

#### 4.4 For Loops

* Say we want to prove the formula sum of 1 to n = $\frac{n(n+1)}{2}$:

```{r}
compute_s_n = function(n){
  x = 1:n
  sum(x)
}
compute_s_n(3)
compute_s_n(100)
```

* For loops allow us to change define the range that a variable takes:
* For loop format looks like: for(i in range of values){operations that use i in that range of values}
* Whatever variable in the range will be set to the end value

Simple Example:
```{r}
for(i in 1:5){
  print(i)
}
i
```

Here is a for loop for the sum problem:
```{r}
m = 25
#create an empty vector
s_n = vector(length=m)
for(n in 1:m){
  s_n[n]=compute_s_n(n)
}
n = 1:m
plot(n,s_n)
lines(n,(n*(n+1))/2)
```
Notice that the line matches the points perfectly

##### Other Functions

* For loops are rarely used because there is usually a more powerful way to do the same task
* Examples are: apply, sapply, tapply, and mapply

---

## Course 2: Data Visualization

### Section 1: Intro to Data Visualization and Distributions

#### 1.1 Intro to Data Visualization

Data Types:  

* Discrete Numerical: numbers from a countably infinite list
* Continuous Numerical: can be any number
* Categorical: data can be broken up into categories (e.g Male and Female, or regions of the US)
* Ordinate: Ordered categorical (can also be numbers) (e.g tall, average, short)

---

#### 1.2 Intro to Distributions

* For categorical data, the distribution simply describes the proportions of each unique category

```{r}
library(dslabs)
data(heights)
head(heights)

prop.table(table(heights$sex))
```

##### Cumulative Density Functions

* When there are more categories, a bar plot is better at describing the distribution.
* Cumulative Distribution Function (CDF) is a function that reports the the proportion of data below a certain value:
$$F(a)=Pr(x \leq a)$$
* The proportion of data that are between two values (a and b) can be calculated using:
$$F(b) - F(a)$$
* If we want to get the proportion of data that is above a certain value:
$$1 - F(a) = Pr(x \geq a)$$  

Here is an example of creating a cdf related for the heights:
```{r}
a <- seq(min(heights$height), max(heights$height), length = 100)    
heightcdf_function <- function(x) {    
  mean(heights$height <= x)
}
heightcdf_values <- sapply(a, heightcdf_function)
plot(a, heightcdf_values)
heightcdf_function(72)
```  


##### Smooth Density Plots  
* Smooth density plots can be thought of as histograms where the bin width is extremely or infinitely small.
* The area under the smooth density curve much be 1
* The area under the curve between two points it the proportion of the data that is between those values.  

##### Normal Distribution

Formula for the Normal Distribution:
$$ Pr(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi}s}e^{-\frac{1}{2}(\frac{x-m}{s})^2}dx$$
where $m$ is the mean, and $s$ is the standard deviation.  

* ~95% of the data is within 2 standard deviations of the mean.

In order to make a normal distribution, first the mean and standard deviation must be calculated. Here are two ways using the heights of the males as the data:
```{r}
library(tidyverse)
library(dslabs)
data(heights)
index = heights$sex=="Male"
x = heights$height[index]

average = sum(x)/length(x)
sd = sqrt(sum((x-average)^2)/length(x))

average2 = mean(x)
sd2 = sd(x)

c(average = average2, sd = sd2)
```

The standard unit of a value, z, tells how many standard deviations a value is away from the mean:
$$z = \frac{x-\mu}{\sigma}$$

You can compute the standard value (z-value) using the scale() function:
```{r}
z = scale(x)
head(z)
```
The probability of the value being less than two standard deviations away from the mean can be calculated using:
```{r}
mean(abs(z) < 2)
```  

The 68-95-99.7 Rule refers to the proportion of the data between 1-2-3 standard deviations from the mean.  

* The pnorm(a, avg, s) is the cdf for the normal distribution (i.e F(a) = pnorm(a, avg, s))
For example, the probability that a male is taller than 70.5 inches:
```{r}
x = heights %>% filter(sex == "Male") %>% pull(height)
1-pnorm(70.5,mean(x),sd(x))
```

* The normal distribution is defined for continuous variables
* In some cases, like the heights case, it is important to discretize the data. In this case, the rounding of some of the data to the nearest inch skews the data to favor integers which discredits the heights that were more specific or in between two integer heights.
* Probabilities are defined for intervals, not specific values. In our heights case, it is useful to use intervals that include exactly one integers.

Here is an example of the heights data with intervals that include a round number (based on data vs. approximation):
```{r}
plot(prop.table(table(x)), xlab = "a = Height in inches", ylab = "Pr(x=a)")

mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)

pnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))
pnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))
pnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))
```

Here is an example of an interval that does not include a round number (based on data vs. approximation):
```{r}
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))
```

---

#### 1.3 Quantiles, Percentiles, and Boxplots

* A quantile is a cutoff point that divides a dataset into intervals with set probability. The *q*th quantile is the value at which q% of the observations are equal to or less than that value.
* quantile(data,q) will find the *q*th quantile of data
* Percentiles are the quantiles that divide a dataset into 100 intervals with 1% probability each. 
* Percentiles can be determined with quantile(data, seq(.01,.99,.01))
* Quartiles divide the dataset into 4 parts with 25% probability each, equal to the 25th (1st), 50th (median), and 75th (3rd) percentiles.
* The summary() function returns the min, quartiles, and max of a dataset
Example with the heights dataset:
```{r}
summary(heights$height)
p = seq(.01,.99,.01)
percentiles = quantile(heights$height, p)
percentiles[names(percentiles) == "25%"]
percentiles[names(percentiles) == "75%"]
quantile(heights$height, .12)
```

##### qnorm()

* qnorm(p, mu, sigma) gives the quantile given the distribution is a normal distribution with mean mu, and sd sigma.
* Note that mu and sigma default to 0 and 1, respectively (standard normal dist)
* qnorm is the inverse function of pnorm:
```{r}
pnorm(-1.96)
qnorm(0.025)
qnorm(pnorm(1.34234252))
```
* qnorm can be used to determine the theoretical quantiles of a dataset.
```{r}
p = seq(.01,.99,.01)
theor_quant = qnorm(p,69,3)
```

##### Quantile-Quantile Plots

* q-q plots are a good way to see if the normal distrubution is a good approximation.
* If the quantiles for the data match the quantile for the normal distrubution, then the normal distrubution is most likely a good approximation.
Example using the heights dataset:
```{r}
index = heights$sex == "Male"
x = heights$height[index]
z = scale(x)
p = seq(.05,.95,.05)
obs_quant = quantile(x,p)
theo_quant = qnorm(p, mean(x),sd(x))
plot(theo_quant, obs_quant)
abline(0,1)
```
Because the q-q plot closely matches the line, the normal distribution is a good approximation.

##### Boxplots

* Boxplots are a 5 number summary: the min, max, 25th, 50th, and 75th quantiles with outliers as independent points
* The box is the 25th, and 75th quantile, the median is the line in the middle, and the edges of the lines are the min and max.
* Interquartile range is the range between 25th and 75th quantiles

#### 1.4 Exploratory Data Analysis
* If a dist is not normal, it cannot be summarized with only the mean and sd. Provide a histogram, smooth density, or boxplot instead.
* A plot can force us to see unexpected results that make us questions the quality or implications of our data.

---

### Section 2: Intro to ggplot2

#### 2.1 Basics of ggplot2

* ggplot2 is part of the tidyverse package. Load using library(tidyverse) or library(ggplot2)
* ggplot2 is generally more intuitive - based on using "building blocks" to create plots
* Works exclusively with data tables with rows being observations and columns variables
* Cheat sheet for ggplot2 functions: Help -> Cheat Sheets -> Data Visualization with ggplot2
* Plots in ggplot2 consist of 3 main components:   
i) Data Component: The data set being summarized  
ii) Geometry Component: The type of plot  
iii) Aesthetic Mapping: Variables mapped to visual cues (e.g axes and color)  

To first create a plot with ggplot2, you need to create a ggplot object (2 ways below):
```{r}
library(tidyverse)
library(dslabs)
data(murders)

p = ggplot(data=murders) # Method 1
p = murders %>% ggplot() # Method 2
class(p)
```

---

#### 2.2 Customizing Plots

* in ggplot2, graphs are created by adding layers. This is done by using +:
* DATA %>% ggplot() + LAYER_1 + LAYER_2 + ... + LAYER_N
* The geometry layer defines the plot type and takes the form geom_X, where X is the plot type
* aes() is used to define aesthetic mappings
* aes() uses variable names from the object component (e.g total rather than murders$total)
* geom_point() creates a scatter plot and requires x and y aesthetic mappings
* geom_text() and geom_label() add text to a scatterplot and require, x, y, and label aesthetic mappings (text just adds the text, label adds a box with the text instead of the points)
* You can add layers with different aesthetic mappings to the same graph  

Example using the murders dataset:
```{r}
murders %>% ggplot() + geom_point(aes(x=population/10^6, total))
```

You can also use a predefined ggplot object:
```{r}
p = ggplot(data=murders)
p + geom_point(aes(population/10^6, total)) + geom_text(aes(population/10^6, total, label = abb))
```  

Here are some ways to tinker with the graph:
```{r}
#Changes Size Of Points
p + geom_point(aes(population/10^6, total), size = 3) +
  geom_text(aes(population/10^6, total, label = abb))

#Moves Text Labels to the Right
p + geom_point(aes(population/10^6, total), size = 3) +
  geom_text(aes(population/10^6, total, label = abb), nudge_x = 1)

#Simplify Code by Adding a Global Aesthetic
p <- murders %>% ggplot(aes(population/10^6, total, label = abb))
p + geom_point(size = 3) +
  geom_text(nudge_x = 1.5)

#Local Aesthetic Overrides Global Aesthetics
p + geom_point(size = 3) +
  geom_text(aes(x = 10, y = 800, label = "Hello there!"))
```

##### Scales, Labels, and Colors

* Convert the x-axis to log scale with scale_x_continuous(trans = "log10") or scale_x_log10() (similar for y-axis)
* Add axis titles with xlab() and ylab()
* Add plot title with ggtitle()
* Add a color mapping by using col argument in aes(). To make all same color, define col outside of aes()
* Add a line with the geom_abline() which takes slope and intercept that have default 1 and 0 respectivly. Change the color with col and the line type with lty.
* Place the line layer before the points to have the points on top of the line, or vice-versa.

Here is the above applied to the murders data set (scaling axes, adding titles/labels to plot/axes, changing the color for the region, adding a line for average murder rate, and adding a legend):
```{r}

p <- murders %>% #Globally define p
  ggplot(aes(population/10^6, total, label = abb)) + #Add points with label
  geom_text(nudge_x = 0.075) + # Slightly move the labels
  scale_x_log10() + #Scale the x-axis
  scale_y_log10() + #Scale the y-axis
  xlab("Population in millions (log scale)") + #Add x-label
  ylab("Total number of murders (log scale)") + #Add y-label
  ggtitle("US Gun Murders in 2010") #Add plot title

r <- murders %>% #Define the murder rate
    summarize(rate = sum(total) / sum(population) * 10^6) %>%
    pull(rate)
p + 
    geom_abline(intercept = log10(r), lty = 2, color = "darkgrey") + #Adding dark gray, dashed line
    geom_point(aes(col = region), size = 3) + #Resize the points, color by region, and add legend
    scale_color_discrete(name = "Region") #Retitle the legend
```

##### Add-on Packages to Support ggplot2

Add-on packages are listed below:
```{r}
library(ggrepel)
library(ggthemes)
```
The ggrepel package includes a function that makes sure text labels do not overlap with each other, and ggthemes adds themes to use. Here's an example of two of the themes:
```{r}
install.packages("ggthemes")
install.packages("ggrepel")
library(ggrepel)
library(ggthemes)
p + theme_economist()
p + theme_fivethirtyeight()
```

Using these packages, we can round off the graph. 
```{r}
murders %>%
    ggplot(aes(population/10^6, total, label = abb)) +
    geom_abline(intercept = log10(r), lty = 2, color = "darkgrey") +
    geom_point(aes(col = region), size = 3) +
    geom_text_repel() +
    scale_x_log10() +
    scale_y_log10() +
    xlab("Population in millions (log scale)") +
    ylab("Total number of murders (log scale)") +
    ggtitle("US Gun Murders in 2010") +
    scale_color_discrete(name = "Region") +
    theme_economist()
```

##### Other Plot Examples

* geom_histogram() creates a histogram. Use **binwidth** argument to change the width of the bins, the **fill** to change the bar fill color, and **col** to change the bar outline color
* geom_density() creates smooth density plots. Change the fill color with the **fill** argument
* geom_qq() creates a q-q plot. This geometry requires the **sample** argument. By default, the data is compared to a standard normal dist. This can be changed with the **dparams** argument
* Plots can be arranged adjacent to each other using the grid.arrange() function from the gridExtra package. First create the plots and save them to objects, the pass the plot objects through grid.arrange()

Histogram Example:
```{r}
data(heights)
p = heights %>%
  filter(sex == "Male") %>%
  ggplot(aes(x=height))

p + geom_histogram(binwidth = 1, fill = "blue", col = "black") +
  xlab("Male heights in inches") +
  ggtitle("Histogram")
```

Smooth Density Example:
```{r}
p = heights %>%
  filter(sex == "Male") %>%
  ggplot(aes(x=height))

p + geom_density(fill = "blue")

heights %>%
ggplot(aes(height, group = sex, color = sex, fill = sex)) +
geom_density(alpha = .2)
```

Q-Q Plot Examples:
```{r}
#Basic qq plot
p = heights %>% filter(sex == "Male") %>%
  ggplot(aes(sample = height))
p + geom_qq()

#qq plot with normal dist
params = heights %>%
  filter(sex == "Male") %>%
  summarize(mean = mean(height), sd = sd(height))
p + geom_qq(dparams = params) + geom_abline()

#qq plots of scaled data against the standard normal
heights %>%
  ggplot(aes(sample = scale(height))) +
  geom_qq() +
  geom_abline()
```

Grid of plots example:
```{r}
library(gridExtra)
p = heights %>% filter(sex == "Male") %>% ggplot(aes(x=height))
p1 = p +geom_histogram(binwidth = 1, fill = "blue", col = "black")
p2 = p +geom_histogram(binwidth = 2, fill = "blue", col = "black")
p3 = p +geom_histogram(binwidth = 3, fill = "blue", col = "black")

grid.arrange(p1,p2,p3, ncol = 3)
```

---

### Section 3: Summarizing with dplyr

#### 3.1 Summarizing with dplyr

* summarize() from the dplyr package computes summary statistics from a data frame and returns a data frame whose column names are defined with the function call.
* summarize() can compute any summary function that operates on vectors and returns a single value, but it cannot operate on functions that return multiple values.

Example of summarize() using heights:
```{r}
data(heights)

s = heights %>%
  filter(sex == "Male") %>%
  summarize(average = mean(height), standard_deviation = sd(height))
s$average
s$standard_deviation

heights %>%
  filter(sex == "Male")%>%
  summarize(median = median(height),
            minimum = min(height),
            maximum = max(height))
```

Example of code that would error when using summarize() on a function that returns multiple values:
```{r, eval = F}
heights %>%
  filter(sex == "Male") %>%
  summarize(range = quantile(height, c(0, 0.5, 1)))
```

##### The Dot Placeholder
* The dot operator allows you to access values stored in data that is being piped using the %>% character. THe dot is a placeholder for the data being passed in throught the pipe. 
* The dot operator allows dplyr functions to return single vectors or numbers instead of only data frames.
* An equivalent way to extract a single column using the pip is us_murder_rate %>% pull(rate)

Example using the murders dataset:
```{r}
data(murders)

#This is incorrect mean murders because it doesn't weight by population
murders = murders %>% mutate(murder_rate = total/population*100000)
summarize(murders, mean(murder_rate))

#Calculate rate using a data frame
us_murder_rate = murders %>%
  summarize(rate = sum(total)/sum(population)*100000)
us_murder_rate

#Using dot placeholder to access rate
us_murder_rate %>% .$rate

#extract rate with one pipe
us_murder_rate = murders %>%
  summarize(rate = sum(total)/sum(population) * 100000) %>%
  .$rate
```


##### Group By Function

* The group_by() function from dplyr converts a data frame to a grouped data frame, creating groups using one more more variables.
* summarize() and some other dplyr functions will behave differently on grouped data frames.
* Using summarize() on a grouped data frame computes the summart statistics for each of the separate groups.

Examples:
```{r}
data(heights)
data(murders)

#Group heights by male and female
heights %>%
  group_by(sex) %>%
  summarize(average = mean(height), standard_deviation = sd(height))

#Group median murders by region
murders = murders %>%
  mutate(murder_rate = total/population*100000)
murders %>%
  group_by(region) %>%
  summarize(mediate_rate = median(murder_rate))
```

##### Sorting Data Tables

* The arrange() function from dplyr sorts a data frame by a given column
* By default, arrange() sorts in ascending order. To instead sort in descending order, use the function desc().
* You can arrange() by multiple levels: within equivalent values of the first level, overservations are sorted by the second level, and so on.
* The top_n() function shows the top results ranked by a given variabel, but the results are not ordered. You can combine top_n() with arrange() to return the top results in order.

Examples using the murders data:
```{r}
murders = murders %>%
  mutate(murder_rate = total/population *100000)

#Arrange by population column
murders %>% arrange(population) %>% head()

#Arrange by murder rate
murders %>% arrange(murder_rate) %>% head()

#Arrange by murder rate in descending order
murders %>% arrange(desc(murder_rate)) %>% head()

#Arrange by region alphabetically, then by murder rate within each region
murders %>% arrange(region, murder_rate) %>% head()

#Show the top 10 states with the highest murder rate, not ordered by rate
murders %>% top_n(10, murder_rate)

#Show the top 10 states with the highest murder rate, ordered by rate
murders %>% arrange(desc(murder_rate)) %>% top_n(10)
```

---

### Section 4: Gapminder Data Set

#### 4.1 Intro to the Gapminder Dataset

* Using the dataset, we will answer question about world health and economics
* This will be done with data visualization

Example from the Gapminder Dataset (Infant mortality in Sri Lanka vs. Turkey):
```{r}
install.packages("tidyverse")
library(dslabs)
library(tidyverse)
data(gapminder)
head(gapminder)

gapminder %>%
  filter(year == 2015 & country %in% c("Sri Lanka", "Turkey")) %>%
  select(country, infant_mortality)
```

Plotting Fertility vs. Life Expectancy in 1962:
```{r}
ds_theme_set()
filter(gapminder, year == 1962) %>%
  ggplot(aes(fertility, life_expectancy, color = continent)) +
  geom_point()
```

---

#### 4.2 Using the Gapminder Dataset

##### Faceting

* Faceting makes multiple side-by-side plots stratified by some variable.
* The *facet_grid()* function allows faceting by up to two variables, with rows faceted by one variable and columns faceted by the other variable. To facet by only one variable, use the dot operator at the other variable.
* The *facet_wrap()* function facets by one variable and automatically wraps the series of plots so they have readable dimensions.
* Faceting keeps the axes fixed across all plots.

Example to compare the fertility rate vs. life expectancy in 1962 and 2012:
```{r}
#facet by continent and year
filter(gapminder, year %in% c(1962, 2012)) %>%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_grid(continent ~ year)

#facet by year
filter(gapminder, year %in% c(1962, 2012)) %>%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_grid(. ~ year)

#facet by year, plots wrapped in multiple rows
years = c(1962, 1970, 1980, 1990, 2000, 2012)
continents = c("Europe", "Asia")
gapminder %>%
  filter(year %in% years & continent %in% continents) %>%
  ggplot(aes(fertility, life_expectancy, col = continent)) +
  geom_point() +
  facet_wrap(~year)
```

##### Time Series Plots

* Time Series Plots have time on the x-axis and a variable of interest on the y-axis
* The geom_line() geometry connects adjacent data points to form a continuous line. A line plot is appropriate when points are regularly spaced, densely packed from a single data series.
* You can plot multiple lines on the same graph. Remember to group or color by a variable so that the lines are plotted independently.
* Labeling is usually preferred over legends. However, legends are easier to make and appear by default. Add a label with geom_text(), specifying the coordinates where the label should appear on the graph.

Single Time Series Example:
```{r}
gapminder %>%
  filter(country == "United States") %>%
  ggplot(aes(year, fertility)) +
  geom_point()

gapminder %>%
  filter(country == "United States") %>%
  ggplot(aes(year, fertility)) +
  geom_line()
```

Multiple Time Series Example:
```{r}
countries = c("South Korea","Germany")
gapminder %>% filter(country %in% countries) %>%
  ggplot(aes(year,fertility, group = country, col = country)) +
  geom_line()
```

Adding Text Labels to Plot:
```{r}
labels = data.frame(country = countries, x = c(1975,1965), y = c(60,72))
gapminder %>% filter(country %in% countries) %>%
  ggplot(aes(year, life_expectancy, col = country)) +
  geom_line() +
  geom_text(data=labels, aes(x,y, label = country), size = 5) +
  theme(legend.position = "none")
```
* x and y are the coordinates of the labels (x1,y1 for SK, and x2,y2 for Germany)

##### Transformations

* Log transformations convert multiplicative changes into additive changes.
* Common transformations are the log base 2 transformation and the log base 10 transformation. The choice of base depends on the range of the data. The natural log is not recommended because the it is difficult to interpret.
* The mode of a distribution is the value with the highest frequency. The mode of a normal dist is the average. A dist can have multiple local modes.
* There are two ways to use log transformations in plots: transform the data before plotting or transform the axes of the plot. Log scales have the advantage of showing the original values as axis labels, while log transformed values east interpretation of intermediate values between labels.
* Scale the x-axis using scale_x_continuous() or scale_x_log10() layers in ggplots2. Similar functions exist for the y-axis.

Example using income:
```{r}
gapminder = gapminder %>%
  mutate(dollars_per_day = gdp/population/365)

#histogram of dollars per day
past_year = 1970
gapminder %>%
  filter(year == past_year & !is.na(gdp)) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = "black")

#repeat with log2 scaled data
gapminder %>%
  filter(year == past_year & !is.na(gdp)) %>%
  ggplot(aes(log2(dollars_per_day))) +
  geom_histogram(binwidth = 1, color = "black")

#repeat with log2 scaled x-axis
gapminder %>%
  filter(year == past_year & !is.na(gdp)) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = "black") +
  scale_x_continuous(trans = "log2")
```

##### Stratify and Boxplot

* Make boxplots stratified by a categorical variable using geom_boxplot()
* Rotate axis labels by changing the theme throught element_text(). You can change the angle and justification of the text labels.
* Consider order your factors by a meaningful value with the reorder() function, which changes the order of factor levels based on a related numeric vector. This is a way to ease comparisons.
* Show the data by adding data points to teh boxplot with a geom_point() layer. This adds information beyond the five-number summary to your plot, but too many data points it can obfuscate your message.

Box plot of GDP by region:
```{r}
gapminder = gapminder %>%
  mutate(dollars_per_day = gdp/population/365)

#number of regions
length(levels(gapminder$region))

#create boxplot and rotate x-axis text
past_year = 1970
p = gapminder %>%
  filter(year == past_year & !is.na(gdp)) %>%
  ggplot(aes(region, dollars_per_day))
p + geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust =1))
```

Example of how the reorder function is used:
```{r}
fac = factor(c("Asia", "Asia", "West", "West", "West"))
levels(fac)

value = c(10, 11, 12, 6, 4)
fac = reorder(fac, value, FUN = mean)
levels(fac)
```

Example combining reorder with boxplots (order by median dollars/day):
```{r}
p = gapminder %>%
  filter(year == past_year & !is.na(gdp)) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%
  ggplot(aes(region, dollars_per_day, fill = continent)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("")
p

#log2 scale y-axis and add points
p + scale_y_continuous(trans = "log2") + geom_point(show.legend = F)
```

##### Comparing Distributions

* Use *intersect()* to find the overlap between two vectors.
* To make boxplots where grouped variables are adjacent, color the boxplot by a factor instead of faceting by that factor. This is a way to ease comparisons.

Example using wealth gap between rich and poor over the years:
```{r}
gapminder = gapminder %>%
  mutate(dollars_per_day = gdp/population/365)
past_year = 1970

west = c("Western Europe", "Northern Europe", "Southern Europe", "Northern America", "Australia and New Zealand")

#facet by west vs. developing
gapminder %>%
  filter(year == past_year, !is.na(gdp)) %>%
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = "black") +
  scale_x_continuous(trans = "log2") +
  facet_grid(.~group)

#facet by west/developing and year
present_year = 2010
gapminder %>%
  filter(year %in% c(past_year, present_year) & !is.na(gdp)) %>%
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
  ggplot(aes(dollars_per_day)) +
  geom_histogram(binwidth = 1, color = "black") +
  scale_x_continuous(trans = "log2") +
  facet_grid(year ~ group)
```

Income distribution of West vs. Developing only with countries with data in both years:
```{r}
#Get countries that have data in both 1970 and 2010
country_list_1 = gapminder %>%
  filter(year == past_year & !is.na(dollars_per_day)) %>% .$country
country_list_2 = gapminder %>%
  filter(year == present_year & !is.na(dollars_per_day)) %>% .$country
country_list = intersect(country_list_1, country_list_2)

#Remake histogram with these countries
gapminder %>%
    filter(year %in% c(past_year, present_year) & country %in% country_list) %>%    # keep only selected countries
    mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
    ggplot(aes(dollars_per_day)) +
    geom_histogram(binwidth = 1, color = "black") +
    scale_x_continuous(trans = "log2") +
    facet_grid(year ~ group)
```

Boxplots of income disparity:
```{r}
p = gapminder %>%
  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%
  mutate(region = reorder(region, dollars_per_day, FUN = median)) %>%
  ggplot() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  xlab("") + scale_y_continuous(trans = "log2")

p +geom_boxplot(aes(region, dollars_per_day, fill = continent)) +
  facet_grid(year ~.)

#Arrange boxplots next to each other by year
p + geom_boxplot(aes(region, dollars_per_day, fill = factor(year)))
```

##### Density Plots

* Change the y-axis of density plots to variable using *..count..* as the y argument.
* The *case_when()* function defines a factor whose levels are defined by a variety of logical operations ot group data.
* Plot stacked density plots using *position = "stack"*
* Define a weight aesthetic mapping to change the relative weights of density plots. For example, this allows weighting of plot by populations rather than number of counties.

Example of smooth density plots:
```{r}
#First get the number of countries in each of the regions (West and Developing)
gapminder %>%
  filter(year == past_year & country %in% country_list) %>%
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>% group_by(group) %>%
  summarize(n = n()) %>% knitr::kable()

#Create the smooth density plot - Variable counts on the y axis
p = gapminder %>%
  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%
  mutate(group = ifelse(region %in% west, "West", "Developing")) %>%
  ggplot(aes(dollars_per_day, y = ..count.., fill=group)) +
  scale_x_continuous(trans = "log2")
p + geom_density(alpha = .2) + facet_grid(year ~.)
```

Using case_when(), the creating a non-weighted, then weighted stacked density plot:
```{r}
#Add group as a factor and group the regions
gapminder = gapminder %>%
  mutate(group = case_when(
    .$region %in% west ~ "West",
    .$region %in% c("Eastern Asia", "South-Eastern Asia") ~ "East Asia",
    .$region %in% c("Caribbean", "Central America", "South America") ~ "Latin America",
    .$continent == "Africa" & .$region != "Northern Africa" ~ "Sub-Saharan Africa",
    TRUE ~ "Others"))

#reorder region factor levels
gapminder = gapminder %>%
  mutate(group = factor(group, levels = c("Others", "Latin America", "East Asia", "Sub-Saharan Africa", "West")))

#Stacked density plot
p = gapminder %>%
  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%
  ggplot(aes(dollars_per_day, fill = group)) +
  scale_x_continuous(trans = "log2")
p + geom_density(alpha = .2, bw = .75, position = "stack") +
  facet_grid(year ~ .)

#Weighted Stacked density plot
gapminder %>%
  filter(year %in% c(past_year, present_year) & country %in% country_list) %>%
  group_by(year) %>%
  mutate(weight = population/sum(population*2)) %>%
  ungroup() %>%
  ggplot(aes(dollars_per_day, fill = group, weight = weight)) +
  scale_x_continuous(trans = "log2") +
  geom_density(alpha = .2, bw = .75, position = "stack") +
  facet_grid(year ~.)
```

##### Ecological Fallacy

* The *breaks* argument allows us to set the location of the axis labels and tick marks.
* The logistic or logit transformation is defined as $f(p) = log(\frac{p}{1-p})$, or the log of odds. This scale us useful for highlighting differences near 0 or near 1 and converts fold changes into constant increases.
* The ecological fallacy is assuming that conclusions made from the average of a group apply to all members of the group.

Example showing income vs. infant survival rate (only shows averages at regional level):
```{r}
gapminder = gapminder %>%
  mutate(group = case_when(
    .$region %in% west ~ "The West",
    .$region %in% "Northern Africa" ~ "Northern Africa",
    .$region %in% c("Eastern Asia", "South-Eastern Asia") ~ "East Asia",
    .$region == "Southern Asia" ~ "Southern Asia", 
    .$region %in% c("Central America", "South America", "Caribbean") ~ "Latin America",
    .$continent == "Africa" & .$region != "Northern Africa" ~ "Sub-Saharan Africa",
    .$region %in% c("Melanesia", "Micronesia", "Polynesia") ~ "Pacific Islands"))

surv_income = gapminder %>%
  filter(year %in% present_year & !is.na(gdp) & !is.na(infant_mortality) & !is.na(group)) %>%
  group_by(group) %>%
  summarize(income = sum(gdp)/sum(population)/365,
                                  infant_survival_rate = 1 - sum(infant_mortality/1000*population)/sum(population))
surv_income %>% arrange(income)

surv_income %>% ggplot(aes(income, infant_survival_rate, label = group, color = group)) +
  scale_x_continuous(trans = "log2", limit = c(.25, 150)) +
  scale_y_continuous(trans = "logit", limit = c(.875, .9981), breaks = c(.85,.9,.95,.99,.995,.998)) +
  geom_label(size = 3, show.legend = F)
```
This plot does not show the whole picture. A plot of each country would tell a more clear story.

### Section 5: Data Visualization Principles

#### 5.1 Data Visualization Principles, Part 1

* Visual cues for encoding data include position, length, angle, area, brightness, and color hue.
* Position and length are the preferred way to display quantities, followed by angles, which are preferred over area. Brightness and color are even harder to quantify but can sometimes be useful.
* Pie charts represent visual cues as both angles and area, while donut charts use only area. Humans are not good a visually quantifying angles and are even worse at quantifying area, therefore pie and donut charts should be avoided in lieu of bar plots. If using a pie chart, use percentages as labels.
* Bar plots represent visual cues as position and length. Humans are good at visual quantifying linear measures, making bar plots a strong alternative to pie or donut charts.
* When using bar plots, always start at 0.
* When using position rather than length, it is not necessary to include 0 (scatterplot, dotplot, boxplot).
* Make sure your visualizations encode the correct quantities. For example, if you are use a plot that relies on circle area, make sure the area (rather than the radius) is proportional to the quantity.
* It is easiest to visually extract information from a plot when categories are ordered by a meaningful value. The exact value on which to order will depend on your data and the message you wist to convey with your plot.
* The degault ordering for categories is alphabetical if the categories are string or by factor level if factors. However, we rarely want alphabetical order.

#### Data Visualization Principles, Part 2

* A dynamite plot - a bar graph of group averages with error bars denoting standard errors - provides almost no information about a distribution.
* By showing the data, you provide viewers extra information about distributions.
* Jitters is adding a smell random shift to each point in order to minimize the number of overlapping points. To add jitter, use the *geom_jitter()* geometry in place of geom_point().
* Alpha blending is making points somewhat transparent, helping to visualize the density of overlapping points. To do this, add an *alpha* argument to the geometry.

Example of a normal plot vs. a jittered plot.
```{r}
#normal dot plot
heights %>% ggplot(aes(sex,height)) + geom_point()

#jittered plot
heights %>% ggplot(aes(sex, height)) + geom_jitter(width = .1, alpha = .2)
```

* Ease comparisons by keeping axes the same when comparing data across multiple plots.
* Align plots vertically to see horizontal changes and vice versa.
* Bar plots are useful for showing one number not not useful for showing distributions.
* Use transformations when warranted to ease visual interpretation.
* The log transformation is useful for data with multiplicative changes. Th logistic transformation is useful for fold changes in odds. The square root transformation is useful for count data.
* When two groups are to be compared, it is optimal to place them adjacent in the plot.
* Use color to encode groups to be compared.
* Consider using a color blind friendly palette.

Example of color blind friendly colors:
```{r}
color_blind_friendly_cols = c("#999999", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
p1 = data.frame(x = 1:8, y = 1:8, col = as.character(1:8)) %>%
  ggplot(aes(x,y, color = col)) +
  geom_point(size = 5)
p1 + scale_color_manual(values = color_blind_friendly_cols)
```

#### 5.3 Data Visualization Part 3

##### Slope Charts

* Consider using a slope chart or Bland-Altman plot when comparing one variable at two different time points, especially for a small number of observations.
* Slop charts use gangle to encode change. Use *geom_line()* to create slop charts. It is useful when comparing a small number of observations.
* The Bland-Altman plot (Tukey mean difference plot, MA plot) graphs the difference between conditions on the y-axis and the mean between conditions on the x-axis. It is more appropriate for large number of observations that slope charts.

Example of a slope chart for life expectancy between 2010 and 2015 in Western countries:
```{r}
west = c("Western Europe", "Northern Europe", "Southern Europe", "Northern America", "Australia and New Zealand")
dat = gapminder %>%
  filter(year %in% c(2010, 2015) & region %in% west & !is.na(life_expectancy) & population > 10^7)

dat %>%
  mutate(location = ifelse(year == 2010, 1, 2),
         location = ifelse(year == 2015 & country %in% c("United Kingdom", "Portugal"),
                           location + .22, location),
         hjust = ifelse(year == 2010, 1, 0)) %>%
  mutate(year = as.factor(year)) %>%
  ggplot(aes(year, life_expectancy, group = country)) +
  geom_line(aes(color=country), show.legend = F) +
  geom_text(aes(x=location, label = country, hjust = hjust), show.legend = F) +
  xlab("") +
  ylab("Life Expectancy")
```

Example of Bland-Altman plot comparing average life expectancy between 2010 and 2015 to the difference of those years :
```{r}
library(ggrepel)
dat %>%
  mutate(year = paste0("life_expectancy_", year)) %>%
  select(country, year, life_expectancy) %>% spread(year, life_expectancy) %>%
  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,
         difference = life_expectancy_2015 - life_expectancy_2010) %>%
  ggplot(aes(average, difference, label = country)) +
  geom_point() +
  geom_text_repel() +
  geom_abline(lty = 2) +
  xlab("Average of 2010 and 2015") +
  ylab("Difference between 2015 and 2010")
```

##### Encoding a Third Variable w/ Vaccines Case Study

* Encode a third categorical variable on a scatterplot using color, hue, or shape. Use the *shape* argument to control shape.
* Encode a continuous third variable using color, intensity, or size
* The *RColorBrewer* package offers several color palettes. Sequential color palettes are best suited for data that span from high to low. Diverging color palettes are best suited for data that are centered and diverge towards high or low values.
* The *geom_tile()* geometry created a grid of colored tiles.
* Position and length are stronger cues than color for numeric values, but color can be appropriate sometimes.

Vaccines Case Study (disease rate per year in CA):
```{r}
data(us_contagious_diseases)
str(us_contagious_diseases)

the_disease = "Measles"
dat = us_contagious_diseases %>%
  filter(!state %in% c("Hawaii", "Alaska") & disease == the_disease) %>%
  mutate(rate = count/population *10000 * 52/weeks_reporting) %>%
  mutate(state = reorder(state,rate))

dat %>% filter(state == "California" & !is.na(rate)) %>%
  ggplot(aes(year, rate)) +
  geom_line() +
  ylab("Cases per 10,000") +
  geom_vline(xintercept=1963, col = "blue")
```

Tile plot of disease rate by state and year:
```{r}
dat %>% ggplot(aes(year, state, fill=rate)) +
  geom_tile(color = "grey50") +
  scale_x_continuous(expand = c(0,0)) +
  scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "Reds"), trans = "sqrt") +
  geom_vline(xintercept = 1963, col = "blue") +
  theme_minimal() + theme(panel.grid = element_blank()) +
  ggtitle(the_disease) +
  ylab("") +
  xlab("")
```

Line plot of measles rate by year and state:
```{r}
avg = us_contagious_diseases %>%
  filter(disease == the_disease) %>% group_by(year) %>%
  summarize(us_rate = sum(count, na.rm = T)/sum(population, na.rm = TRUE)*10000)

dat %>%
  filter(!is.na(rate)) %>%
  ggplot() +
  geom_line(aes(year, rate, group = state), color = "grey50",
            show.legend = F, alpha = .2, size = 1) +
  geom_line(mapping = aes(year, us_rate), data = avg, size = 1, col = "black") +
  scale_y_continuous(trans = "sqrt", breaks = c(5,25,125,300)) +
  ggtitle("Cases per 10,000 by state") +
  xlab("") +
  ylab("") +
  geom_text(data = data.frame(x = 1955, y = 50),
            mapping = aes(x,y,label = "US average"), color = "black") +
  geom_vline(xintercept = 1963, color = "blue")
```

##### Significant Figures

* *round()* and *signif()* can be used to reduce the number of significant figures. Also options(digits = n) will globally set the sig figs.

## Course 3: Probability

### Section 1: Discrete Probability

#### 1.1 Intro to Discrete Proabability

*The probability of an event is the proportion of time the even occurs when we repeat the experiment independently under the same conditions:
$$ Pr(A) = probability of event A $$
* An event is defined as an outcome that can occur when something happens by chance.
* We can determine probabilities related to discrete variables (picking a red bead, choosing 48 Democrats and 52 Republicans from 100 likely votes) and continuous variables (height over 6 ft).

##### Monte Carlo Simulations

* Monte Carlo simulation model the probability of different outcomes by repeating a random process a large enough number of times that the results are similar to what would be observed if the process were repeated forever.
* The *sample()* function draws random outcomes from a set of options. (This is done without replacement by default, set replacement = TRUE to make it with replacement).
* The *replicate()* function repeats lines of code a set number of times. It is used with sample() and similar functions to run Monte Carlo simulations. 

Example of a Monte Carlo Simulation:
```{r}
beads = rep(c("red","blue"), times = c(2,3)) #create an container with 2 red and 3 blue beads
sample(beads,1) #sample one bead at random

B = 10000 #number of times to draw a bead
events = replicate(B, sample(beads,1))
tab = table(events)
tab
prop.table(tab)
```

##### Setting Seeds

* Use set.seed(x) to set the seed to x
* add sample.kind = "Rounding" to get old seeds (from 3.5), this will be used in this course as the seeds are from 3.5.

##### Mean Function

* Applying the mean() function to a logical vector returns the proportion of true elements. It is very common to use the mean functions in this way to calculate probabilities.

Example using the beads:
```{r}
beads = rep(c("red","blue"), times = c(2,3))
mean(beads == "blue")
```

##### Probability Distributions

* The probability distribution for a variable describes the probability of observing each possible outcome.
* For discrete categorical variables, the probability distribution is defined by the proportions for each group.


##### Indepenence

* Conditional Probabilities compute the probability that an event occurs given information about dependent events. For example, the probability of drawing a second king given the first draw is a king is:

$$Pr(Card 2 is a King|Card 1 is a King) = \frac{3}{51}$$  
* If two events, A and B, are independent, $Pr(A|B) = Pr(A)$.
* To determine the probability of multiple events occurring, we use the multiplication rule:
$$Pr(A and B and C) = Pr(A) \cdot Pr \cdot Pr(C)$$  
* The multiplication rule for dependent events considers the conditional probability of both events occurring:
$$Pr(A and B) = Pr(A) \cdot Pr(B|A)$$  
* We can expand the multiplication rule for dependent events to more than 2 events:
$$Pr(A and B and C) = Pr(A) \cdot Pr(B|A) \cdot Pr(C|A and B)$$  

#### 1.2 Combinations and Permutations

* *paste()* joins two strings and inserts a space between them.
* *expand.grid()* gives the combinations of 2 vectors or lists.
* *permutations(n,r)* from the gtools packages list the different ways that r items can be selected from a set of n options when order matters.
* *combinations(n,r)* from the gtools packages list the different ways that r items can be selected from a set of n options when order does not matter.

Examples:
```{r}
number = "Three"
suit = "Hearts"
paste(number, suit)

paste(letters[1:5], as.character(1:5))

expand.grid(pants = c("blue","black"), shirt = c("white","grey", "plaid"))
```

Generating a deck of cards:
```{r}
suits = c("Diamonds", "Clubs", "Hearts", "Spades")
numbers = c("Ace", "Deuce", "Three", "Four", "Five", "Six", "Seven", "Eight", "Nine", "Ten", "Jack", "Queen", "King")
deck = expand.grid(number = numbers, suit = suits)
deck = paste(deck$number, deck$suit)

#probability of drawing a king
kings = paste("King", suits)
mean(deck %in% kings)
```

Permutations and Combinations (all phones numbers without repeating digits):
```{r}
library(gtools)
permutations(5,2) #ways to choose 2 numbers in order from 1:5
all_phone_numbers = permutations(10,7, v = 0:9)
n = nrow(all_phone_numbers)
index = sample(n,5)
all_phone_numbers[index,]

permutations(3,2) #order matters
combinations(3,2) #order doesn't matter
```

Probability of drawing a second king given that a first one has been drawn:
```{r}
hands = permutations(52, 2, v = deck)
first_card = hands[,1]
second_card = hands[,2]
sum(first_card %in% kings)
sum(first_card %in% kings & second_card %in% kings)/sum(first_card %in% kings)
```

Probability of a natural 21 in blackjack:
```{r}
aces <- paste("Ace", suits)
facecard <- c("King", "Queen", "Jack", "Ten")
facecard <- expand.grid(number = facecard, suit = suits)
facecard <- paste(facecard$number, facecard$suit)

hands = combinations(52,2,v=deck)

#probability of natural 21 given that ace is first in combination
mean(hands[,1] %in% aces & hands[,2] %in% facecard)

#probability of natural 21 checking for both ace first and ace second
mean((hands[,1] %in% aces & hands[,2] %in% facecard)|(hands[,2] %in% aces & hands[,1] %in% facecard))
```

Monte Carlo simulation of natural 21 in blackjack
```{r}
hand = sample(deck, 2)
hand

B = 10000
results = replicate(B, {
  hand = sample(deck,2)
  (hand[1] %in%aces & hand[2] %in% facecard)  |(hand[2] %in% aces & hand[1] %in% facecard)
})
mean(results)
```


##### The Birthday Problem

* *duplicated()* takes a vector and returns a vector of the same length with TRUE for any elements that have appeared previously in that vector.
* We can compute the probability of shared birthdays in a group of people by modeling birthdays as random draws from the numbers 1 through 365. We can then use this sampling model of birthdays to run a Monte Carlo simluation to estimate the probability of shared birthdays.

```{r}
n = 50
bdays = sample(1:365, n, replace = T)
any(duplicated(bdays))

B = 10000
results = replicate(B , {
  bdays = sample(1:365, n, replace = T)
  any(duplicated(bdays))
})
mean(results)
```

* Some functions automatically apply element-wise to vectors, such as sqrt() and *.
* However, other functions do not operate element-wise by default. This includes functions we define ourselves.
* The functions *sapply(x, f)* allows any other functions f to be applied element-wise to the vector x.
* The probability of an event happening is 1 minus the probability of that event not happening:
$$Pr(event) = 1-Pr(no event)$$
* We can compute the probability of shard birthdays mathematically:
$$Pr(SharedBirthdays) = 1-Pr(noSharedBirthdays) = 1 - (1 \cdot \frac{364}{365} \cdot \frac{363}{365} \cdot \frac{362}{365} ... \cdot \frac{365-n+1}{365})$$

Function for Bday Problem using Monte Carlo:
```{r}
compute_prob = function(n, B=10000) {
  same_day = replicate(B, {
    bdays = sample(1:365, n, replace = T)
    any(duplicated(bdays))
  })
  mean(same_day)
}
n = seq(1,60)
prob = sapply(n, compute_prob)
plot(n, prob)
```

Computing Birthday Problem probabilities with sapply:
```{r}
exact_prob = function(n){
  prob_unique = seq(365, 365-n+1)/365
  1 - prod(prob_unique)
}
eprob = sapply(n, exact_prob)

plot(n, prob)
lines(n, eprob, col = "red")
```

* The larger the number of Monte Carlo replicates, B, the more accurate the estimate but the longer it takes to do.
* Determining the appropriate size for B can require advances statistics.
* One practical approach is to try many sizes for B and look for sizes that provide stable estimates. 

Estimating a practical value of B:
```{r}
B = 10^seq(1,5,len = 100)
compute_prob = function(B, n = 22){
  same_day = replicate(B, {
    bdays = sample(1:365, n , replace = T)
    any(duplicated(bdays))
  })
  mean(same_day)
}

prob = sapply(B, compute_prob)
plot(log10(B), prob, type = "l")
```

#### 1.3 Addition Rule and Monty Hall

* The addition rules states that the probability of event A or event B happening is the probability of event A plus the probability of event B minus the probability of both events A and B happening together:
$$Pr(AorB) = Pr(A)+Pr(B)-Pr(AandB)$$
* Note that (A or B) is equivalent to (A|B)

##### Monty Hall Problem
* Monte Carlo simulations can be sued to simulate random outcomes, which makes them useful when exploring ambiguous or less intuitive problems like the Monty Hall problem.
* In the Monty Hall problem, contestants choose one of three doors that may contain a prize. Then, one of the doors that was not chosen by the contestant and does not contain a prize is revealed. The contestant can then choose whether to stick with the original choice or switch to the remaining unopened door.
* Although it may seem intuitively like the contestant has a 1 in 2 chance of winning regardless of whether tehy stick or swich, Monte Carlo simulations demonstrate the the actual probability of winning is 1 in 3 with the stick strategy and 2 in 3 with the switch strategy.

Monte Carlo Simulation for Stick Strategy:
```{r}
B = 10000
stick = replicate(B, {
  doors = as.character(1:3)
  prize = sample(c("car","goat","goat"))
  prize_door = doors[prize == "car"]
  my_pick = sample(doors,1)
  show = sample(doors[!doors %in% c(my_pick, prize_door)],1)
  stick = my_pick
  stick == prize_door
})
mean(stick)
```

Monte Carlo Simulation for Switch Strategy:
```{r}
B = 10000
switch = replicate(B, {
  doors = as.character(1:3)
  prize = sample(c("car","goat","goat"))
  prize_door = doors[prize == "car"]
  my_pick = sample(doors,1)
  show = sample(doors[!doors %in% c(my_pick, prize_door)], 1) 
  switch = doors[!doors%in%c(my_pick, show)] 
  switch == prize_door
})
mean(switch)
```

### Section 2: Continuous Probability

#### 2.1 Continuous Probability

##### Cumulative Distribution Functions

* The cumulative distribution function (CDF) is a distribution function for continuous data x that report the proportion of the data below a for all values of a:
$$F(a)=Pr(x \leq a)$$
* The CDF is the probability distribution function for continuous variables. For example, to determine the probability that a mal student is taller than 70.5 inches given a vector of male heights, x, we can use the CDF:

$$Pr(x \gt 70.5) = 1-Pr(x \leq 70.5) = 1 - F(70.5)$$

* The probability that an observation is between two values, a and b, $is F(b) - F(a)$

Example of CDF (Male heights less than 70 inches):
```{r}
x = heights %>% filter(sex=="Male") %>% pull(height)
F = function(a) mean(x<=a)
1-F(70)
```

##### Theoretical Distribution

* *pnorm(a,avg,s)* gives the value of the cumulative distribution function F(a) for the normal distribution defined by the average and standard deviation.
* We say that a random quantity is normally distributed with average avg and standard deviation s if the approximation *pnorm(a,avg,s)* holds for all values of a.
* If we are willing to use the normal approximation for height, we can estimate the distribution simply from the mean and standard deviation of our values.
* If we treat the height data as discrete rather than categorical, we see that the data are not very useful because integer values are more common due to rounding. This is called discretization.
* With rounded data, the normal approximation is particularly useful when computing probabilities of intervals of length one that include exactly one integer.

Using pnorm():
```{r}
x = heights %>% filter(sex=="Male") %>% pull(height)
1 - pnorm(70.5, mean(x), sd(x))

#plot dist of exact heights
plot(prop.table(table(x)), xlab = "a = Height in inches", ylab = "Pr(x = a)")

#probabilities in actual data over length 1 ranges containing an integer
mean(x <= 68.5) - mean(x <= 67.5)
mean(x <= 69.5) - mean(x <= 68.5)
mean(x <= 70.5) - mean(x <= 69.5)

#probabilities in normal approx match well
pnorm(68.5, mean(x), sd(x)) - pnorm(67.5, mean(x), sd(x))
pnorm(69.5, mean(x), sd(x)) - pnorm(68.5, mean(x), sd(x))
pnorm(70.5, mean(x), sd(x)) - pnorm(69.5, mean(x), sd(x))

#probabilities in actual data over other ranges don't match normal approx as well
mean(x <= 70.9) - mean(x <= 70.1)
pnorm(70.9, mean(x), sd(x)) - pnorm(70.1, mean(x), sd(x))
```

##### Probability Density

* The probability of a single value is not defined for a continuous distribution.
* The quantity with the most similar interpretation to the probability of the single value is the probability density function f(x).
The probability density function is defined such the the integral of f(x) over a ranges gives the CDF of that range:

$$F(a) = Pr(X \geq a) = \int_{-\infty}^a f(x)dx$$

* In R, the probability density function for the normal distribution is given by *dnorm()*. We will see uses of dnorm() in the future.
* Note that dnorm() gives the density function and pnorm() gives the distribution function, which is the integral of the density function.

Plotting a PDF:
```{r}
x = seq(-4,4,length = 100)
data.frame(x, f=dnorm(x)) %>%
  ggplot(aes(x,f)) +
  geom_line()
```

##### Continuous Monte Carlo Simulations

* *rnorm(n, average, s)* generates n random numbers from the normal distribution with average avg and standard deviation s.
* By generating random numbers from the normal distribution, we can simulate height data with similar properties to our dataset. Here we generate simulation data using the normal distribution.

```{r}
x = heights %>% filter(sex == "Male") %>% pull(height)

#generate simulated height data using normal dist - both datasets with n observations
n = length(x)
avg = mean(x)
s = sd(x)
simulated_heights = rnorm(n,avg,s)

#plot dist of simulated heights
data.frame(simulated_heights = simulated_heights) %>%
  ggplot(aes(simulated_heights)) +
  geom_histogram(color="black", binwidth = 2)
```

Monte Carlo Simulation of tallest person over 7ft:
```{r}
B = 10000
tallest = replicate(B, {
  simulated_data = rnorm(800, avg, s)
  max(simulated_data)
})
mean(tallest >= 7*12)
```

##### Other Continuous Distributions

* You may encounter other continuous distributions (Student t, chi-squared, exponential, gamma, beta, etc...)
* R provides function for density(d), quantile(q), probability dist(p), and random number generation(r) for many of these dists.
* Each dist has a matching abbreviation (for example norm() or t()) that is paired with the related function abbreviation to create appropriate functions.
For example, use rt() to generate random numbers for a Monte Carlo simulation using the Student t dist.

### Section 3: Random Variables, Sampling Models, and the Central Limit Theorem

#### 3.1 Random Variables and Sampling Models

##### Random Variables

* Random variables are numeric outcomes resulting from random processes.
* Statistical inference offers a framework for quantifying uncertainty due to randomness.

Modeling a Random Variable Example:
```{r}
beads = rep(c("red","blue"), times= c(2,3))
x = ifelse(sample(beads,1)=="blue", 0, 1)

ifelse(sample(beads,1)=="blue", 0, 1)
ifelse(sample(beads,1)=="blue", 0, 1)
ifelse(sample(beads,1)=="blue", 0, 1)
```

##### Sampling Models

* A sampling model models the random behavior of a process such as the sample of draws from an urn.
* The probability distribution of a random variable is the probability of the observed value falling in any given interval.
* We can define a CDF $F(a)=Pr(S \leq a)$ to answer questions related to the probability of S being in any interval.
* The average of many draws of a random variable is called its *expected value*.
* The standard deviation of many draws of a random variable is called its *standard error*.

Monte Carlo Simulation: Chance of Casino Losing Money on Roulette:
Building the Sampling Model:
```{r}
#sampling model 1: define urn then sample
color = rep(c("Black","Red","Green"), c(18,18,2)) #define the urn for the sampling model
n = 1000
X = sample(ifelse(color=="Red",-1,1), n, replace = T)
X[1:10]

#sampling model 2: define urn inside sample function by noting probabilities
x = sample(c(-1,1), n, replace = T, prob = c(9/19, 10/19))
S = sum(x)
S
```

Using the results to estimate probn of casino losing money:
```{r}
n = 1000 #number of roulette players
B = 10000 #number of Monte Carlo experiments
S = replicate(B, {
  X = sample(c(-1,1),n,replace = T, prob = c(9/19,10/19)) #simulate 1000 spins
  sum(X) #determine total profit
})
mean(S)
sd(S)
mean(S<0) #probability of casino losing money
```

Plotting a histogram of observed values of S as well as normal curve:
```{r}
s = seq(min(S), max(S), length = 100)
normal_density = data.frame(s=s, f=dnorm(s,mean(S), sd(S)))
data.frame(S=S) %>%
  ggplot(aes(S, ..density..)) +
  geom_histogram(color="black", binwidth = 10) +
  ylab("Probability") +
  geom_line(data = normal_density, mapping = aes(s,f), color="blue")
```

##### Distributions vs. Probability Distributions

* A random variable X has a probability distribution F(a) that defines $Pr(X \leq a)$ over all values of a.
* Any list of numbers has a distributions. The probability distribution function of a random variable is defined mathematically and does not depend on a list of numbers.
* The results of a Monte Carlo simulation with a large enough number of observations will approximate the probability dist of X.
* If a random variable is defined as draws from an urn:
  + The probability dist function of the random variable is defined as the dist of the list of values in the urn
  + The expected value of the RV is the average of the values in the urn.
  + The standard error of one draw of the random variable is the standard deviation of the values in the urn.

##### Notation for Random Variables

* Capital letters denote random variables ($X$) and lowercase letters denote observed values ($x$).
* In the notation $Pr(x = x)$, we are asking how frequently the random variable $X$ is equal to the value $x$. For example, if $x = 6$, this statment becomes $Pr(X=6)$.

##### Central Limit Theorem

* The Central Limit Theorem (CLT) says the the dist of the sum of a random variable is approximated by a normal dist.
* THe expected value of a random variable $E[X] = \mu$, is the average of the values in the urn. This represents the expectation of one draw.
* The standard error of one draw of a random variable is the standard deviation of the values in the urn.
* The expected values of the sum of draws is the number of draws times the expect value of the random variable.
* The standard error of the sum of independent draws of a random variable is the square root of the number of draws times the standard deviation of the urn.

Equations:

* Expected value of a random variable:

$$ap+b(1-p)$$

* Expected value of the sum of n draws of a random variable:

$$n \cdot (ap+b(1-p))$$

* Standard deviation of an urn with two values:

$$|b-a| \sqrt{p(1-p)}$$

* Standard error of the sum on n draws of a random variable:

$$\sqrt{n} \cdot |b-a| \sqrt{p(1-p)}$$

#### Central Limit Theorem, Continuted

##### Averages and Proportions

* The expected value or standard error of an RV multiplied by a constant is that constant times the original value:

$$E[aX]=a \mu$$

$$SE[aX]=a \sigma$$
* The standard deviation of the average of multiple draws from an urn is the standard deviation divided by the square root of the number of draws:

$$\frac{\sigma}{\sqrt{n}}$$

* The sum of multiple draws of a random variables:

$$E[nX]=n \mu$$

$$SE[nX] = \sqrt{n} \sigma$$

* The sum of multiple different random variables:

$$E[X_1 + X_2 + ... +X_n]=\mu_1 + \mu_2 +...+\mu_n$$

$$SE[X_1 + X_2 + ... +X_n] = \sqrt{\sigma_1^2 + \sigma_2^2 +...+ \sigma_n^2}$$
* Note that $\sigma^2$ is also known as the Variance.
* If X is normally dist and *a* and *b* are non-random constants, then $aX+b$ is also a normally distributed random variable.

##### Law of Large Numbers

* The law of large numbers states that as n increases, the standard error of the average of a random variable decreases. In other words, when n is large, the average of the draws converges on the average of the urn.
* The law of large numbers is also known as the law of averages.
* The law of averages only applies when n is very large and events are independent. It is often misused to make predictions about an event being "due" because it has happened less frequently than expected in a small sample size.
* The sample size required for the Central Limit Theorem and Law of Large Numbers to apply differs based on the probability of success.
  + If the probability of success is high, then relatively few observations are needed.
  + As the probability of success decreases, more observations are needed.
* If the probability of success is extremely low, such as winning a lottery, then the Central Limit Theorem may not apply even with extremely large sample sizes. The normal dist is not a good approx in these cases, and other dists such as the Poisson dist may be more appropriate.

#### 4.1 The Big Short

* Interest rates for loans are set using the probability of loan defaults to calculate a rate that minimizes the probability of losing money.
* We can define the outcomes of loans as a random variable. We can also define the sum of outcomes of many loans as a random variable.
* The Central Limit Theorem can be applies to fit a normal dist to the sum of profits over many loans. We can use properties of the normal dist to calculate the interest rate needed to ensure a certain probability of losing money for a given probability of default.

Interest Rate Sampling Model:
```{r}
n = 1000
loss_per_foreclosure = -200000
p = .02
defaults = sample(c(0,1),n,prob=c(1-p,p),replace = T)
sum(defaults*loss_per_foreclosure)
```

Interest Rate Monte Carlo Simulation:
```{r}
B = 10000
losses = replicate(B, {
  defaults = sample(c(0,1), n, prob = c(1-p,p), replace=T)
  sum(defaults*loss_per_foreclosure)
})
```

Plotting Expected Losses:
```{r}
data.frame(losses_in_millions=losses/10^6) %>%
  ggplot(aes(losses_in_millions)) +
  geom_histogram(binwidth = .6, col = "black")
```

Expected Value and Standard Error of the Sum of 1000 Loans:
```{r}
n*(p*loss_per_foreclosure + (1-p)*0) #Expected Value
sqrt(n)*abs(loss_per_foreclosure)*sqrt(p*(1-p)) #Standard Error
```

* We can calculate the amount, $x$, to add to each loan so that the expected value is 0 using the equation $lp + x(1-p) = 0$. Note that this equation is the definition of expected value given a loss per foreclosure $l$ with foreclosure probability $p$ and profit $x$ if there is no forecloseure (probability $1-p).

* We solve for the following equation and calculate $x$:

$$x=-\frac{lp}{1-p}$$

This number is then divided by the loan amount to find the interest rate (example $180,000 loan):
```{r}
x = -loss_per_foreclosure*p/(1-p)
x
rate = x/180000
rate
```

##### Calculating Interest Rate for 1% Probability of Losing Money

We want to calculate the value of $x$ for which $Pr(S<0)=0.01$. The expected value $E[S]$ of the sum of $n=1000$ loans given our definitions of $x$, $l$, and $p$ is:

$$\mu_s=(lp+x(1-p)) \cdot n$$

And the standard error of the sum of $n$ loans, $SE[S], is:

$$\sigma_s = |x-l| \sqrt{np(1-p)}$$

Because we know the definition of a Z-score is $Z = \frac{x-\mu}{\sigma}$, we know that $Pr(S<0)=Pr(Z<-\frac{\mu}{\sigma})$. Thus, $Pr(S<0)=0.01$ equals:

$$Pr(Z<\frac{-(lp+x(1-p))n}{(x-l) \sqrt{np(1-p)}})=0.01$$

z = qnorm(0.01) gives us the value of $z$ for which $Pr(Z \leq z) =0.01$, meaning:

$$z = \frac{-(lp+x(1-p))n}{(x-l) \sqrt{np(1-p)}}$$

Solving for $x$ gives:

$$x = -l \frac{np-z \sqrt{np(1-p)}}{n(1-p)+z \sqrt{np(1-p)}}$$
Calculating interest rate for 1% probability of losing money:
```{r}
l = loss_per_foreclosure
z = qnorm(0.01)
x <- -l*( n*p - z*sqrt(n*p*(1-p)))/ ( n*(1-p) + z*sqrt(n*p*(1-p)))
rate = x/180000
rate
loss_per_foreclosure*p + x*(1-p) #Expected value of the profit per loan
n*(loss_per_foreclosure*p+x*(1-p)) #Expected profit over n loans
```

Monte Carlo Simulation for 1% Prob of Losing Money:
```{r}
B=100000
profit = replicate(B, {
  draws = sample(c(x,loss_per_foreclosure), n, prob = c(1-p,p), replace = T)
  sum(draws)
})
mean(profit) #expected value of profit over n loans
mean(profit<0) # probability of losing money
```

* The Central Limit Theorem states that the sum of the independent draws of a random variable follows a normal dist. However,  when the draws are not independent, this assumption does not hold.
* If an event changes the probability of default for all borrowers, then the probability of the bank losing money changes. 
* Monte Carlo simulations can be used to model teh effects of unknown changes in the prob of default.

Expected Value with Higher Default Rate and Interest Rate:
```{r}
p = .04
loss_per_foreclosure = -200000
r = .05
x = r*180000
loss_per_foreclosure*p + x*(1-p)
```

Equations of Probability:
we can define our desired probability of losing money, $z$, as:

$$Pr(S<0)=Pr(Z<-\frac{E[S]}{SE[S]})=Pr(Z<z)$$

If $\mu$ is the expected value of the urn (one loan) and $\sigma$ is the standard deviation of the urn (one loan), the $E[S]=n\mu$ and $SE[S]=\sqrt{n}\sigma$.
As in the previous video, we define the probability of losing money $z=0.01$. In the first equation, we can see that:

$$z=-\frac{E[S]}{SE[S]}$$

It follows that:

$$z=-\frac{n\mu}{\sqrt{n}\sigma}=-\frac{\sqrt{n}\mu}{\sigma}$$

To find the value of $n$ for which $z$ is less than or equal to our desired value, we take $z \leq -\frac{\sqrt{n}\mu}{\sigma}$ and solve for $n$:

$$n \geq \frac{z^2\sigma^2}{\mu^2}$$
Calculating Number of Loans for Desired Probability of Losing Money:
```{r}
z <- qnorm(0.01)
l <- loss_per_foreclosure
n <- ceiling((z^2*(x-l)^2*p*(1-p))/(l*p + x*(1-p))^2)
n    # number of loans required
n*(loss_per_foreclosure*p + x * (1-p))    # expected profit over n loans
```

Monte Carlo Simulation with Known Default Probability:
```{r}
B <- 10000
p <- 0.04
x <- 0.05 * 180000
profit <- replicate(B, {
    draws <- sample( c(x, loss_per_foreclosure), n, 
                        prob=c(1-p, p), replace = TRUE) 
    sum(draws)
})
mean(profit)
```

Monte Carlo Simulation with Unknown Default Probability:
This Monte Carlo simulation estimates the expected profit given an unknown probability of default $0.03 \leq p \leq 0.05$, modeling the situation where an event changes the probability of default for all borrowers simultaneously. Note that your results will differ from the video because the seed is not set.
```{r}
p <- 0.04
x <- 0.05*180000
profit <- replicate(B, {
    new_p <- 0.04 + sample(seq(-0.01, 0.01, length = 100), 1)
    draws <- sample( c(x, loss_per_foreclosure), n, 
                        prob=c(1-new_p, new_p), replace = TRUE)
    sum(draws)
})
mean(profit)    # expected profit
mean(profit < 0)    # probability of losing money
mean(profit < -10000000)    # probability of losing over $10 million
```

## Course 4: Inference and Modelling

* Information gathered from a small random sample can be used to infer characteristics of the entire population.
* Opinion polls are useful when asking everyone in a population is impossible.
* A common use for opinion polls is determining voter preferences in political elections for the purposes of forecasting election results.
* The spread of a poll is the estimated difference between support of two candidates or options.

### Section 1: Parameters and Estimates

* The task of statistical inference is to estimate an unknown population parameter using observed data from a sample.
* In a sampling model, the collection of elements in the urn is called the population.
* A parameter is a number that summarized data for an entire population
* An estimate is a summary of the observed data about a parameter that we believe is informative. It is a data-driven guess of the population parameter.
* We want to predict the proportion of the blue beads in the urn, the parameter $p$. The proportion of red beads in the urn is $1-p$ and the spread is $2p-1$.
* The sample proportion is a random variable. Sampling gives random results drawn from the population distribution.

Example of using a function to take a poll from a Red/Blue urn:
```{r}
take_poll(25)
```

* Many common data science tasks can be framed as estimating a parameter from a sample.
* We illustrate statistical inference by walking through the process to estimate $p$. From the estimate of $p$, we can easily calculate an estimate of the spread, $2p-1$.
* Consider the random variable $X$ that is 1 if a blue bead is chosen and 0 if a red bead is chosen. The proportions of blue beads in $N$ draws is the average of the draws $X_1,...,X_N$.
* $\bar{X}$ is the sample average. In statistics, a bar on top of a symbol denote the average.$\bar{X}$ is a random variable because it is the average of random draws - each time we take a sample, $\bar{X}$ is different:

$$\bar{X} = \frac{X_1+X_2+...+X_N}{N}$$

*The number of blue beads in N draws, $N\bar{X}$, is $N$ times the proportion of values in the urn. However, we do not know the true proportion: we are trying to estimate this parameter, $p$.
* A poll taken in advance of an election estimates $p$ for that moment, not for election day.
* In order to predict election results, forecasters try to use early estimates of $p$ to predict $p$ on election day. We discuss some approaches in later sections.
* When interpreting values of $\bar{X}$, it is important to remember that $\bar{X}$ is a random variable with an expected value and standard error that represents the sample proportion of positive events.
*The expected value $\bar{X}$ is the parameter of interest $p$. This follows from the fact that $\bar{X}$ is the sum of independent draws of a random variable times a constant $1/N$.

$$E(\bar{X}) = p$$

*As the numbers of draws $N$ increases, the standard error of our estimate $\bar{X}$ decreases. The standard error of the average of $\bar{X}$ over $N$ draws is:

$$SE(\bar{X}) = \sqrt{p(1-p)/N}$$

* In theory, we can get more accurate estimates of $p$ by increasing $N$. In practice, there are limits on the size of N due to costs, as well as other factors we discuss later.
* We can also use other random variable equations to determine the expected value of the sum of draws $E(S)$ and standard error of the sum of draws $SE(S)$.

$$E(S) = Np$$

$$SE(S) = \sqrt{Np(1-p)}$$

### Section 2: The Central Limit Theorem in Practice

* Because $\bar{X}$ is the sum of random draws divided by a constant, the distribution of $\bar{X}$ is approximately normal.
* We can convert $\bar{X}$ to a standard normal random variable, $Z$:

$$Z = \frac{\bar{x} - E(\bar{x})}{SE(\bar{x})}$$
* The probability that $\bar{X}$ is within .01 of the actual value of $p$ is:

$$Pr(Z \leq .01/\sqrt{p-(1-p)/N})-Pr(Z \leq .01/\sqrt{p(1-p)/N})$$

* The Central Limit Theorem (CLT) still works if $\bar{X}$ is used in place of p. This is called a plug-in estimate. Hats over values denote estimates. Therefore:

$$\hat{SE}(\bar{X})=\sqrt{\bar{X}(1-\bar{X})/N}$$

* Using CLT, the probability that $\bar{X}$ is within .01 of the actual value of $p$ is:

$$Pr(Z \leq .01/\sqrt{bar{X}-(1-bar{X})/N})-Pr(Z \leq .01/\sqrt{bar{X}(1-bar{X})/N})$$
Computing Probability of $\bar{X}$ being within .01 of p:
```{r}
X_hat = .48
se = sqrt(X_hat*(1-X_hat)/25)
pnorm(.01/se)-pnorm(-.01/se)
```

* The margin of error is defined as two times the standard error of the estimate $\bar{X}$.
* There is about a 95% chance that $\bar{X}$ will be within two standard errors of the actual parameter $p$.

##### Monte Carlo Simulation for the CLT

* We can run Monte Carlo simulations to compare with theoretical results assuming a value of $p$.
* In practice, $p$ is unknown. We can corroborate theoretical results by running Monte Carlo simulations with one or several values of $p$.
* One practical choice for $p$ when modelling $\bar{X}$, the observed value of $\hat{X}$ in a sample.

Monte Carlo Simulation using a set value of p:
```{r}
p = .45 #unknown p to estimate
N = 1000

#simulate one poll of size N and determine x_hat
x = sample(c(0,1), size = N, replace = TRUE, prob = c(1-p,p))
x_hat = mean(x)

#Simulate B polls of size N and determine average x_hat
B = 10000 #number of replicates
N = 1000  #sample size per replicate
x_hat = replicate(B,{
                  x = sample(c(0,1), size = N, replace = T, prob = c(1-p,p))
                  mean(x)
                  })
```

Histogram and QQ-plot of Monte Carlo results:
```{r}
p1 = data.frame(x_hat = x_hat) %>%
  ggplot(aes(x_hat)) +
  geom_histogram(binwidth = .005, color = "black")
p2 = data.frame(x_hat = x_hat) %>%
  ggplot(aes(sample=x_hat)) +
  stat_qq(dparams = list(mean = mean(x_hat), sd = sd(x_hat))) +
  geom_abline()+
  ylab("X_hat") +
  xlab("Theoretical normal")
grid.arrange(p1,p2,nrow = 1)
```

##### Spread

* The spread between two outcomes with probabilities $p$ and $1-p$ is $2p-1$.
* The expected value of the spread is $2\bar{X}-1$.
* The standard error of the spread is $2\hat{SE}(\bar{X})$.
* The margin of error of the spread is 2 times the margin of error of $\bar{X}$.

##### Why not run a very large poll?

* An extremely large poll would theoretically be able to predict election results almost perfectly.
* These sample sizes are not practical. In addition to cost concerns, polling doesn't reach everyone in the population (eventual voters) with equal probability, and it also may include data from outside our population (people who will not end up voting).
* These systematic errors in polling are called bias. We will learn more about bias in the future.

Plotting margin of error in an extremely large poll over a range of values of p:
```{r}
N = 100000
p = seq(.35, .65, length = 100)
SE = sapply(p,function(x) 2*sqrt(x*(1-x)/N))
data.frame(p=p,SE=SE) %>%
  ggplot(aes(p,SE)) +
  geom_line()
```

### Section 3: Confidence Intervals and p-Values

### Confidence Intervals

* We can use statistical theory to compute the probability that a given interval contains the true parameter $p$.
* 95% confidence intervals are intervals constructed to have a 95% chance of including $p$.The margin of error is approximately a 95% confidence interval.
* The start and end of these confidence intervals are random variables.
* To calculate any size confidence interval, we need to calculate the value $z$ for which $Pr(-z\leq Z\leq z)$ equals the desired confidence. For example, a 99% confidence interval requires calculation $z$ for $Pr(-z \leq\Z \leq z) = .99$.
* For a confidence interval of size q, we solve for $z = 1- \frac{1-q}{2}$.
* To determine a 95% confidence interval, use z = qnorm(.975). This value is slightly smaller than 2 times the standard error.

geom_smooth confidence interval example:
```{r}
data("nhtemp")
data.frame(year = as.numeric(time(nhtemp)), temperature = as.numeric(nhtemp)) %>%
  ggplot(aes(year, temperature)) +
  geom_point() +
  geom_smooth() +
  ggtitle("Average Yearly Temperatures in New Haven")
```

Monte Carlo simulation of confidence intervals:
```{r}
p = .45
N = 1000
X = sample(c(0,1), size = N, replace = T, prob = c(1-p,p))
X_hat = mean(X)
SE_hat = sqrt(X_hat*(1-X_hat)/N)
c(X_hat - 2*SE_hat, X_hat + 2*SE_hat)
```

Solving for z with qnorm:
```{r}
z = qnorm(.995)
pnorm(qnorm(.995))
pnorm(qnorm(1-.995))
pnorm(z)-pnorm(-z)
```

* We can run a Monte Carlo simulation to confirm that a 95% confidence interval contains the true value of $p$ 95% of the time.
* A plot of confidence intervals from this simulation demonstrates that most intervals include $p$, but roughly 5% of intervals miss the true value of $p$.

Monte Carlo simulation (result should be roughly .95 because this is a .95 confidence interval):
```{r}
B = 10000
inside = replicate(B, {
  X = sample(c(0,1), size = N, replace = T, prob = c(1-p,p))
  X_hat = mean(X)
  SE_hat = sqrt(X_hat*(1-X_hat)/N)
  between(p, X_hat - qnorm(.975)*SE_hat, X_hat + qnorm(.975)*SE_hat)
})
mean(inside)
```

* The 95% confidence intervals are random, but $p$ is not random.
* 95% refers to the probability that the random interval falls on top of $p$.
* It is technically incorrect to state that $p$ has a 95% chance of being in between two values because that implies that $p$ is random.

#### Power

* If we are trying to predict the results of an election, then a confidence intervale that includes a spread of 0 (a tie) is not helpful.
* A confidence interval that includes a spread of 0 dows not imply a close election, it means the sample size is too small.
* Power is the probability of detecting an effect when there is a true effect to find. power increases as sample size increases, because larger sample size means smaller standard error.

Confidence interval for the spread with sample size of 25:
```{r}
N = 25
X_hat = .48
(2*X_hat - 1) +c(-2,2)*2*sqrt(X_hat*(1-X_hat)/sqrt(N))
```

### p-Values

* The null hypothesis is the hypothesis that there is no effect. In this cast, the null hypothesis is that the spread is 0, or $p=.5$. (could also be $2p-1 > 0$)
* The p-value is the probability of detecting an effect of a certain size or larger when the null hypothesis is true.
* We can convert the probability of seeing an observed value under the null hypothesis into a standard normal random variable. We compute the value of $z$ that corresponds to the observed result, and then use that $z$ to compute the p-value.
* If a 95% confidence interval does not include our observed value, then the p-value must be smaller than .05.
* It is preferable to report confidence intervals instead of p-values, as confidence intervals give information about the size of the estimate and p-values do not.

Computing p-value for observed spread of .02:
```{r}
N = 100
z = sqrt(N) *.02/.5
1 - (pnorm(z)-pnorm(-z))
```

* The p-value is the probability of observing a value as extreme or more extreme than the result given that the null hypothesis is true.
* In the context of normal dist, this refers to the probability of observing a Z-score whose abs value is as high or higher than the Z-score of interest.
* Suppose we want to find the p-value of an observation 2 standard deviations larger than the mean. This means we are looking for anything with $|z| \geq 2$.
* Recall that, by default, pnorm() gives the CDF for a normal dist with a mean of $\mu = 0$ and standard deviation $\sigma = 1$. To find p-values for a given z-score, $z$, in a normal dist with mean $\mu$ and standard deviation $\sigma$, use 2*p(1-pnorm(z,mu,sigma)) instead.

### Section 4: Statistical Models

##### Poll Aggregators

* Poll aggregators combine the results of many polls to simulate polls with a large sample size and therefore generate more precise estimates than individual polls.
* Polls can be simulated with a Monte Carlo simulation and used to construct an estimate of the spread and confidence intervals.
* The actual data science exercise of forecasting elections involves more complex statistical modeling, but these underlying ideas still apply.

Simulating polls:
```{r}
d = .039
Ns = c(1298, 533, 1342, 897, 774, 254, 812, 324, 1291, 1056, 2172, 516)
p = (d+1)/2

#calculate confidence intervals of the spread
confidence_intervals = sapply(Ns, function(N){
  X = sample(c(0,1), size = N, replace = T, prob = c(1-p,p))
  X_hat = mean(X)
  SE_hat = sqrt(X_hat*(1-X_hat)/N)
  2*c(X_hat, X_hat - 2*SE_hat, X_hat + 2*SE_hat) - 1
})

#generate a data frame storing results
polls = data.frame(poll = 1:ncol(confidence_intervals),
                   t(confidence_intervals), sample_size = Ns)
names(polls) = c("poll", "estimate","low","high","sample_size")
polls
```

Calculating the spread of combined polls
```{r}
d_hat = polls %>%
  summarize(avg = sum(estimate*sample_size)/sum(sample_size)) %>%
  .$avg
p_hat = (1+d_hat)/2
moe = 2*1.96*sqrt(p_hat*(1-p_hat)/sum(polls$sample_size))
round(d_hat*100,1)
round(moe*100,1)
```

##### Pollsters and Multilevel Models

* Different poll aggregators generate different models election results from the same poll data. This is because they use different statistical models.
* We will use actual polling data about the popular vote from the 2016 presidential election to learn the principles of statistical modeling.

##### Poll Data and Pollster Bial

* We analyze real 2016 US polling data organized by FiveThirtyEight. We start by using reliable national polls taken within a week before the election to generate an urn model.
* Consider $p$ the proportion voting for Clinton and $1-p$ the proportion voting for Trump. We are interested in the spread $d=2p-1$.
* Poll results are a random normal variable with expected value of the spread $d$ and standard error $2\sqrt{p(1-p)/N}$.
* Our initial estimate of the spread did not include the actual spread. Part of the reason is that different pollsters have different numbers of polls in our dataset, and each pollster has bias.
* Pollster bias reflects the fact that repeated polls by a given pollster have an expected value different from the actual spread and different from other pollsters. Each pollster has a different bias.
* The urn model does not account for pollster bias. We will develop a more flexible data-driven model that can account for effects like bias.

Generating Simulated Poll Data:
```{r}
library(dslabs)
data(polls_us_election_2016)
names(polls_us_election_2016)

#keep only national polls from week before election with a grade considered reliable
polls = polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-10-31" & (grade %in% c("A+","A","A-","B+")|is.na(grade)))

#add spread estimate
polls = polls %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

#compute estimated spread for combined polls
d_hat = polls %>%
  summarize(d_hat = sum(spread*samplesize)/sum(samplesize)) %>%
  .$d_hat

#compute margin of error
p_hat = (d_hat+1)/2
moe = 1.96*2*sqrt(p_hat*(1-p_hat)/sum(polls$samplesize))

#histogram of the spread
polls %>%
  ggplot(aes(spread)) +
  geom_histogram(color="black", binwidth = .01)
```

Investigating poll data and pollster bias:
```{r}
#number of polls per pollster in week before election
polls %>% group_by(pollster) %>% summarize(n())

#plot results by pollster with at least 6 polls
polls %>% group_by(pollster) %>%
  filter(n() >= 6) %>%
  ggplot(aes(pollster, spread)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#standard errors within each pollster
polls %>% group_by(pollster) %>%
  filter(n() >= 6) %>% summarize(se = 2*sqrt(p_hat*(1-p_hat) /median(samplesize)))
```

##### Data-Driven Models

* Instead of using an urn model where wach polls is a random draw from the same distribution of voters, we instead define a model using an urn that contains poll results from all possible pollsters.
* we assume the expected value of this model is the actual spread $d = 2p-1$.
* Our new standard error $\sigma$ not factors in pollster-to-pollster variability. It can no longer be calculated from $p$ or $d$ and is an unknown parameter.
* The central limit theorem still works to estimate the sample average of many polls $X_1,...,X_N$ because the average of the sum of many random variable is a normally distributed random variable with expected value $d$ and standard error $\sigma/\sqrt{N}$.
* We can estimate the unobserved $\sigma$ as the sample standard deviation, which is calculated with the sd function.

95% Interval using Latest Polls from each Pollster:
```{r}
#collect last results before the election for each pollster
one_poll_per_pollster = polls %>% group_by(pollster) %>%
  filter(enddate == max(enddate)) %>%  #keep latest poll
  ungroup()

#histogram of spread estimates
one_poll_per_pollster %>%
  ggplot(aes(spread)) + geom_histogram(binwidth = .01)

#construct 95% confidence interval
results = one_poll_per_pollster %>%
  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
  mutate(start = avg - 1.96*se, end = avg +1.96*se)
round(results*100,1)
```

### Section 5: Bayesian Statistics

* In the urn model, it does not make sense to talk about the probability of $p$ being greated than a certain value because $p$ is a fixed value.
* With Bayesian statistics, we assume that $p$ is in fact random, which allows us to calculate probabilities related to $p$.
* Hierarchical models describe variability at different levels and incorportate all these levels into a model for estimating $p$.

#### Bayes' Theorem

* Bayes' Theorem stats that the probability of event A happening given event B is equal to the probability of both A and B divided by the probability of event B:

$$ Pr(A|B) = \frac{Pr(B|A)Pr(A)}{Pr(B)}$$

* Bayes' Theorem shows that a test for a very rare disease with  have a high percentage of false positives even if the accuracy of the test is high.

##### Equations for Cystic Fibrosis test probabilities:

In these probabilities, + represents a positive test, - represents a negative test, $D=0$ indicates no disease, and $D=1$ indicates the disease is present.

Probability of have the disease given a positive test: $Pr(D=1|+)$

99% test accuracy when disease is present: $Pr(+|D=1)=.99$

99% test accuracy when disease is absent: $Pr(-|D=0)=.99$

Rate of cystic fibrosis: $Pr(D=1)=0.00025$

Bayes' Theorem can be applied like this:

$$Pr(D=1|+) = \frac{Pr(+|D=1)*Pr(D=1)}{Pr(+)}$$
$$Pr(D=1|+) = \frac{Pr(+|D=1)*Pr(D=1)}{Pr(+|D=1)*Pr(D=1)+Pr(+|D=1)*Pr(D=0)}$$
Substituting in known values from above, we obtain that there is a $.02$ chance that the disease is present given a positive test.

Monte Carlo Simulation:
```{r}
prev = .00025 #disease prevalence
N = 100000 #number of tests
outcome = sample(c("Disease","Healthy"),N,replace = T, prob = c(prev, 1-prev))

N_D = sum(outcome == "Disease") #number with disease
N_H = sum(outcome == "Healthy") #number healthy

#for each person, randomly determine if the test is + or -
accuracy = .99
test = vector("character", N)
test[outcome=="Disease"] = sample(c("+","-"), N_D, replace = T, prob = c(accuracy,1-accuracy))
test[outcome=="Healthy"] = sample(c("-","+"), N_H, replace = T, prob = c(accuracy,1-accuracy))

table(outcome,test)
```

##### Bayes' in Practice

* The techniques we have used up until now are referred to as frequentist statistics as they consider only the frequency of outcomes in a  dataset and do not include any outside information. Frequentist statistics allow us to compute confidence intervals and p-values.
* Frequentist statistics can have problems when sample sizes are small and when the data are extreme compared to historical results.
* Bayesian statistics allows prior knowledge to modify observed results, which alters our conclusion about event probabilities.

##### The Hierarchical Model

* Hierarchical models use multiple levels of variability to model results. They are hierarchical because values in the lower levels of the model are computed using values from higher levels of the model.
* We model baseball player batter average using a hierarchical model with two levels of variability:
  + $p \sim N(\mu,\tau)$ describes player-to-player variability in natural ability to hit, which has a mean $\mu$ and a standard deviation $\tau$.
  + $Y|p \sim N(p,\sigma)$ describes a player's observed batting average given their ability $p$, which has a mean $p$ and a standard deviation $\sigma = \sqrt{p(1-p)/N}$. This represents variability due to luck.
  + In Bayesian hierarchical models, the first level is called the prior distribution and the second level is called the sampling distribution.
* The posterior distribution allows us to compute the probability distribution of $p$ given that we have observed data $Y$.
* By the continuous version of Bayes' rule, the expected value of the posterior distribution $p$ given $Y=y$ is a weighted average between the prior mean $\mu$ and the observed data $Y$:

$$E(p|y) = B\mu + (1-B)Y$$ 
where $$B=\frac{\sigma^2}{\sigma^2 + \tau^2}$$

* The standard error of the posterior distribution $SE(p|Y)^2$ is $\frac{1}{1/\sigma^2 + 1/\tau^2}$. Note that you will need to take the square root of both sides to solve for the standard error.
* The Bayesian approach is also known as shrinking. When $\sigma$ is large, $B$ is close to 1 and our prediction of $p$ shrinks towards the mean $\mu$. When $\sigma$ is small, $B$ is close to zero and our prediction of $p$ is more weighted towards the observed data $Y$.

  
### Section 6: Election Forecasting

* In our model:
  + The spread $d \sim N(\mu,\tau)$ describes our best guess in the absence of polling data. We set $\mu = 0$ and $\tau = .035$ using historical data
  + The average of observed data $\bar{X} | d \sim N(d,\sigma)$ describes randomness due to sampling and the pollster effect.
* Because the posterior distribution is normal, we can report a 95% credible interval that has a 95% chance of overlapping the parameter using $E(p|Y)$ and $SE(p|Y)$.
* Given an estimate of $E(p|Y)$ and $SE(p|Y)$, we can use pnorm to compute the probability that $d>0$.
* It is common to see a general bias that affects all pollsters in the same way. This bias cannot be predicted or measured before the election. We will include a term in later models to account for this variability.

Definition of results object:
```{r}
polls = polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-10-31" &
         (grade %in% c("A+","A","A-","B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton/100-rawpoll_trump/100)

one_poll_per_pollster = polls %>% group_by(pollster) %>% filter(enddate == max(enddate)) %>% ungroup()

results = one_poll_per_pollster %>%
  summarize(avg = mean(spread), se = sd(spread)/sqrt(length(spread))) %>%
  mutate(start = avg - 1.96*se, end = avg + 1.96*se)
```

Computing the posterior mean, standard error, credible interval, and probability:
```{r}
mu = 0
tau = .035
Y = results$avg
sigma = results$se
B = sigma^2/(sigma^2+tau^2)
posterior_mean = B*mu + (1-B)*Y
posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2))

posterior_mean
posterior_se


#95% credible interval
posterior_mean + c(-1.96,1.96)*posterior_se

#probability of d>0
1 - pnorm(0,posterior_mean,posterior_se)
```

#### Mathematical Representations of Models

* If we collect several polls with measured spreads $X_1,...,X_j$ with a sample size of $N$, these random variables have expected value $d$ and standard error $2\sqrt{p(1-p)/N}$
* We represent each measurement as $X_{i,j} = d + b + h_i + \epsilon_{i,j}$, where:
  + The index $i$ represents the different pollsters
  + The index $j$ represents the different polls
  + $X_{i,j}$ is the $j$th poll by the $i$th pollster
  + $d$ is the actual spread of the election
  + $b$ is the general bias affecting all pollsters
  + $h_i$ represents the house effect for the $i$th pollster
  + $\epsilon_{i,j}$ represents the random error associated with the $i,j$th poll.
* The sample average is now $\bar{X} = d+b+\frac{1}{N}\sum\limits_{i=1}^{N} X_i$
* The standard error of the general bias $\sigma_b$ does not get reduced by averaging multiple polls, which increases teh variability of our final estimate.

Simulated data with $X_j = d+\epsilon_j$:
```{r}
J = 6
N = 2000
d = .021
p = (d+1)/2
X = d +rnorm(J,0,2*sqrt(p*(1-p)/N))
X
```

Simulated data with $X_j = d+\epsilon_{i,j}$:
```{r}
I = 5
J = 6
N = 2000
d = .021
p = (d+1)/2
X = sapply(1:I, function(i){
  d + rnorm(J,0,2*sqrt(p*(1-p)/N))
})
X
```

Simulated data with $X_{i,j} = d + h_i + \epsilon_i,j$:
```{r}
I = 5
J = 6
N = 2000
d = .021
p = (d+1)/2
h = rnorm(I,0,.025) #assume standard error of pollster-to-pollster variability is .025
X = sapply(1:I, function(i){
  d + h[i]+ rnorm(J,0,2*sqrt(p*(1-p)/N))
})
```

Calculating Probability of $d>0$ with general bias:
```{r}
mu = 0
tau = .035
sigma = sqrt(results$se^2 + .025^2) #sigma includes an estimate of the variability
Y = results$avg
B = sigma^2/(sigma^2 + tau^2)

posterior_mean = B*mu + (1-B)*Y
posterior_se = sqrt(1/(1/sigma^2+1/tau^2))

1-pnorm(0,posterior_mean,posterior_se)
```

##### Predicting the Electoral College

* In the US election, each state has a certain number of votes that are won all-or-nothing based on the popular vote result in that state (with minor exceptions not discussed here).
* We use left_join() function to combine the number of electoral votes with our poll results.
* For each state, we apply a Bayesian approach to generate an Election Day $d$. We keep our prior simply by assuming an expected value of 0 and standard deviation based on recent history of .02.
* We can run a Monte Carlo simulation that for each iteration simulated poll results in each state using that state's average and standard deviation, awards electoral votes for each state to Clinton if the spread is greater than 0, then compares the number of electoral votes won to the number of votes required to win the election (over 269).
* If we run a Monte Carlo simulation for the electoral college without accounting for the general bias, we overestimate Clinton's chances of winning at over 99%.
* If we include a general bias term the estimated probability of Clinton winning decreases significantly.

Top 5 States by Electoral Votes:
```{r}
results_us_election_2016 %>% arrange(desc(electoral_votes)) %>% top_n(5, electoral_votes)
```

Computing the average and standard deviation for each state:
```{r}
results = polls_us_election_2016 %>%
  filter(state != "U.S." &
           !grepl("CD", "state") &
           enddate >= "2016-10-31" &
           (grade %in% c("A+", "A", "A-", "B+") | is.na(grade))) %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  group_by(state) %>%
  summarize(avg = mean(spread), sd = sd(spread), n = n()) %>%
  mutate(state = as.character(state))

#10 closest races
results %>% arrange(abs(avg))

#joining electoral college votes and results
results = left_join(results,results_us_election_2016,by="state")

#states with no polls
results_us_election_2016 %>% filter(!state %in% results$state)

#assigns sd to states with just one poll as median of other sd values
results = results %>%
  mutate(sd=ifelse(is.na(sd),median(results$sd,na.rm=T),sd))
```

Calculating the Posterior Mean and Posterior Standard Error:
```{r}
mu = 0
tau = .02
results %>% mutate(sigma = sd/sqrt(n),
                   B = sigma^2/(sigma^2+tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)))%>%
  arrange(abs(posterior_mean))
```

Monte Carlo simulation of Election Night results:
```{r}
mu = 0 
tau = .02
clinton_EV = replicate(1000, {
  results %>% mutate(sigma = sd/sqrt(n),
                   B = sigma^2/(sigma^2+tau^2),
                   posterior_mean = B*mu + (1-B)*avg,
                   posterior_se = sqrt(1/(1/sigma^2 + 1/tau^2)),
                   simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                   clinton = ifelse(simulated_result>0, electoral_votes,0)) %>% #award votes if Clinton wins state
    summarize(clinton = sum(clinton)) %>% #total votes for Clinton
    .$clinton +7 #7 votes for RI and DC
})
mean(clinton_EV >269) #269 votes win election

#histogram of outcomes
data.frame(clinton_EV) %>%
  ggplot(aes(clinton_EV)) +
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = 269)
```

Monte Carlo Simlation including general bias:
```{r}
mu <- 0
tau <- 0.02
bias_sd <- 0.03
clinton_EV_2 <- replicate(1000, {
    results %>% mutate(sigma = sqrt(sd^2/(n) + bias_sd^2),    # added bias_sd term
                        B = sigma^2/ (sigma^2 + tau^2),
                        posterior_mean = B*mu + (1-B)*avg,
                        posterior_se = sqrt( 1 / (1/sigma^2 + 1/tau^2)),
                        simulated_result = rnorm(length(posterior_mean), posterior_mean, posterior_se),
                        clinton = ifelse(simulated_result > 0, electoral_votes, 0)) %>%    # award votes if Clinton wins state
        summarize(clinton = sum(clinton)) %>%    # total votes for Clinton
        .$clinton + 7    # 7 votes for Rhode Island and DC
})
mean(clinton_EV_2 > 269)    # over 269 votes wins election

#histogram
data.frame(clinton_EV_2) %>%
    ggplot(aes(clinton_EV-2)) +
    geom_histogram(binwidth = 1) +
    geom_vline(xintercept = 269)
```

##### Forecasting

* In polls results, $p$ is not fixed over time. Variability within a single pollster comes from time variation.
* In order to forecast, our model must include a bias term $b_t$ to model the time effect.
* Pollsters also try to estimate $f(t)$, the trend of $p$ given time $t$ using a model like: 

$$Y_{i,j,t} = d+b+h_j+b_t+f(t)+\epsilon_{i,j,t}$$

* Once we decide on a model, we an use historical data and current data to estimate the necessary parameters to make predictions.

Variability across one pollster:
```{r}
#select all national polls by one pollster
one_pollster = polls_us_election_2016 %>%
  filter(pollster == "Ipsos"& state == "U.S.") %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)

#the observed standard error is higher than theory predicts
se = one_pollster %>%
  summarize(empirical = sd(spread),
            theoretical = 2*sqrt(mean(spread)*(1-mean(spread))/min(samplesize)))
se

#the distribution of the data is not normal
one_pollster %>% ggplot(aes(spread)) +
  geom_histogram(binwidth = .01, color = "black")
```

Trend across time for several pollsters:
```{r}
polls_us_election_2016 %>%
  filter(state == "U.S." & enddate >= "2016-07-01") %>%
  group_by(pollster) %>%
  filter(n() >= 10) %>%
  ungroup() %>%
  mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
  ggplot(aes(enddate, spread)) +
  geom_smooth(method = "loess", span = 0.1) +
  geom_point(aes(color = pollster), show.legend = FALSE, alpha = 0.6)
```

Plotting raw percentages over time:
```{r}
polls_us_election_2016 %>%
  filter(state=="U.S.", enddate >= "2016-07-01") %>%
  select(enddate, pollster, rawpoll_clinton, rawpoll_trump) %>%
  rename(Clinton = rawpoll_clinton, Trump = rawpoll_trump) %>%
  gather(candidate, percentage, -enddate, -pollster) %>%
  mutate(candidate = factor(candidate, levels = c("Trump", "Clinton"))) %>%
  group_by(pollster) %>%
  filter(n() >= 10) %>%
  ungroup() %>%
  ggplot(aes(enddate, percentage, color=candidate)) +
  geom_point(show.legend = FALSE, alpha = .4) +
  geom_smooth(method = "loess", span = .15) +
  scale_y_continuous(limits = c(30,50))
```

##### The t-Distribution

* In models where we must estimate two parameters, $p$ and $\sigma$, the Central Limit Theorem can result in overconfident confidence intervals for sample sizes smaller then approximately 30.
* If the population data are known to follow a normal distribution, theory tells us how much larger to make the confidence intervals to account for estimation of $\sigma$.
* Given $s$ as an estimate of $\sigma$, then $Z = \frac{\bar{X}-d}{s/\sqrt{N}}$ follows a t-distribution with $N-1$ degrees of freedom.
* Degrees of freedom determine the weight of the tails of the distribution. Small values of degrees of freedom lead to increased probabilities of extreme values.
* We can determine confidence intervals using the t-distribution instead of the normal distribution by calculating the desired quantile with the function qt().

Calculating 95% confidence intervals with the t-dist:
```{r}
z = qt(.975, nrow(one_poll_per_pollster) - 1)
one_poll_per_pollster %>%
  summarize(avg = mean(spread), moe = z * sd(spread)/sqrt(length(spread))) %>%
  mutate(start = avg - moe, end = avg + moe)

#quantile from t-dist vs normal dist
qt(.975,14) #14 dof
qnorm(.975)
```


### Section 7: Association Tests

##### Fisher's Exact Test

* We learn how to determine the probability that an observation is due to random variability given categorical, binary, or ordinal data.
* Fisher's exact test determines the p-value as the probability of observing an outcome as extreme or more extreme than the observed outcome given the null dist.
* Data from a binary experiment are often summarized in two-by-two tables.
* The p-value can be calculated from a two-by-two table using Fisher's exact test with the function fisher.test().

Research funding rates example:
```{r}
data(research_funding_rates)
research_funding_rates

#compute totals that were successful or not successful
totals = research_funding_rates %>%
  select(-discipline) %>%
  summarize_all(funs(sum)) %>%
  summarize(yes_men = awards_men,
            no_men = applications_men - awards_men,
            yes_women = awards_women,
            no_women = applications_women - awards_women)

#compare percentage of men/women with awards
totals %>% summarize(percent_men = yes_men/(yes_men + no_men),
                     percent_women = yes_women/(yes_women + no_women))
```

Two-by-Two table and p-value for the Lady Tasting Tea problem:
```{r}
tab = matrix(c(3,1,1,3),2,2)
rownames(tab) = c("Poured Before", "Poured After")
colnames(tab) = c("Guessed Before", "Guessed After")
tab

#p-value calculation with Fisher's Exact Test
fisher.test(tab, alternative = "greater")
```

##### Chi-Squared Tests

* If the sums of the rows and the sums of the columns in the two-by-two table are fixed, then the hypergeometric distribution and Fisher's exact test can be used. Otherwise, we must use the Chi-Squared test.
* The chi-squared test compares the oberserved two-by-two table to the two-by-two table expected by the null hypothesis and asks how likely it is that we see a deviation as large as observed or larger by chance.
* The function chisq.test() takes a two-by-two table and returns the p-value from the chi-squared test.
* The odds ration states how many times larger the odds of an outcome are for one group relative to another group.
* A small p-value does not imply a large odds ration. If a finding has a small p-value but also a small odds ratio, it may not be a practically significant or scientifically significant finding.
* Because the odds ratio is a ratio of ratios, there is no simple way to use the Central Limit Theorem to compute confidence intervals. There are advanced methods for computing confidence intervals for odds ratios that we do not discuss here.

Chi-squared Test:
```{r}
#compute overall funding rate
funding_rate = totals %>%
  summarize(percent_total = (yes_men + yes_women) / (yes_men + no_men + yes_women + no_women)) %>%
  .$percent_total
funding_rate

#construct two-by-two table for observed data
two_by_two = tibble(awarded = c("no","yes"),
                    men = c(totals$no_men, totals$yes_men),
                    women = c(totals$no_women, totals$yes_women))
two_by_two

#compute null hypothesis two-by-two table
tibble(awarded = c("no","yes"),
       men = (totals$no_men + totals$yes_men) * c(1-funding_rate, funding_rate),
       women = (totals$no_women + totals$yes_women) * c(1-funding_rate, funding_rate))

#chi-sq test
chisq_test <- two_by_two %>%
  select(-awarded) %>%
  chisq.test()
chisq_test$p.value
```

Odds Ratio:
```{r}
odds_men = (two_by_two$men[2] / sum(two_by_two$men)) /
  (two_by_two$men[1] / sum(two_by_two$men))
odds_women = (two_by_two$women[2] / sum(two_by_two$women)) /
  (two_by_two$women[1] / sum(two_by_two$women))

odds_men/odds_women
```

p-value and odds ratio responses to increasing sample size:
```{r}
two_by_two %>%
  select(-awarded) %>%
  mutate(men = men*10, women = women*10) %>%
  chisq.test()
```


## Course 5: Productivity Tools
    
### Section 2: Basic Unix

#### Section 1: Introduction to Unix

* The terminal helps to organize files in a system
* On windows, use the Git bash program
* We refer to all the files, folder, and programs (executables) on your computer as the **filesystem**.
* Your filesystem is organized as a series of nested folders each containing files, folders, and executables.
* In Unix, folders are referred to as directories and directories that are inside other directories are often referred to as subdirectories.
* The home directory is where all your stuff is kept. There is a hierarchical nature to the file system.
* The working directory is the current location.
* Each terminal window has a working directory associated with it.
* The **"pwd"** command will display your working directory. The "/" symbol separates directories, while the first "/" at the beginning of the path stands for the root directory. When a path starts with "/", it is a "full path", which finds the current directory from the root directory.
* "~" means the home directory
* Can auto-complete commands and file names with Tab
* "\" can be used to concatenate (e.g. Course\ 1 is Course 1)
* **WARNING** mv doesn't ask to confirm and could overwrite files, rm is permanent
* Can use move command to 

##### Unix Commands

```
ls #list dir content
mkdir folder_name #create directory called folder_name
rmdir folder_name #remove and empty directory as long as it is empty
rem -r folder_name # remove fir that is not empty, "r" stands for recursive

cd #change directory
../ #two dots represents parent directory
. #single dot represents working directory
cd ~/projects #concatenate with forward slashes
cd ../.. #change to two parent layers beyond
cd - #whatever dir you were before
cd #return to the home dir

mv path-to-file path-to-destination-directory #moves file
rm filename-1 filename-2 filename-3 #removes files
mv old-name new-name #If in correct directory, use this to rename files (can combine to move and rename)
cp path-to-file path-to-destination #copies file to destination

less file-name #lets you quickly look at contents of a file (can use q to exit and arrows to navigage)
```

Downloading data to a directory using R-studio (use getwd() to find working directory and then use relative paths):
```
getwd()
url = "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
dest_file = "Course 5/murders/data/murders.csv"
download.file(url,destfile=dest_file)
```

Wrangle the data:
```
murders = read_csv("Course 5/murders/data/murders.csv")
murders = murders %>% mutate(region = factor(region), rate = total/population *10^5)
save(murders, file = "Course 5/murders/rda/murders.rda")
```

Saving figures to a file:
```
murders %>% mutate(abb = reorder(abb, rate)) %>%
  ggplot(aes(abb,rate)) +
  geom_bar(width = .5, stat = "identity", color = "black") +
  coord_flip()

ggsave("Course 5/murders/figs/barplot.png")
```
### Section 3: R Markdown

* Can use CTRL-ALT-I to create code block
* Can use echo = FALSE to make it now show the result of the code but still run
* Can add descriptions to code block (e.g ```{r pressure-summary})
* Can output as html, pdf, word, or github document

### Section 4: Git and Github

* Version control allows us to go back to previous versions of files
* Git(hub) can be collaborative (Pull Request can be used to request changes in code)
* Github makes it easy to share code and reports
* 4 different areas: Working Directory, Staging Area, Local Repository, Upstream Repository
* Clone an existing upstream repository (copy repo url from clone button, and type "git clone <url>"), and all three local stages are the same as the upstream remote
* The working directory is the same as the working directory in Rstudio. When we edit files we only change the files in this place.
* **git status**: tells how the files in the working directory are related to the files in other stages
* edits in the staging area are not tracked by the version control system by default -  we add a file to the staging area by **git add* command.
* **git commit**: to commit files from the staging area to the local repository, we need to add a message stating what we are doing by *git commit -m "something"**
* **git log**: keeps track of all the changes we have made to the local repository.
* **git push** allows moving from the local repository to upstream repository, only if you have the permission.
* **git fetch**: update local repository to be like the upstream repository, from upstream to local.
* **git merge**: make the updated local sync with the working directory and staging area.
* To change everything in one shot (from upstream to working dir), use **git pull** (equivalent to **git fetch + git merge**).

Adding files using the command line:
```
pwd
mkdir git-example
cd git-example
git clone https://github.com/ConnorBruce/murders.git
cd murders
ls
git status
echo "test" >> new-file.txt
echo "temporary" >> tmp.txt
git add new-file.txt
git status
git commit -m "adding a new file" 
git status
echo "adding a second line" >> new-file.txt
git commit -m "minor change to new-file" new-file.txt
git status
git add
git log new-file.txt
git push
git fetch
git merge
```

![Git Commands](C:\Users\conno\OneDrive\Documents\Data Science Certificate\HarvardxDataScienceCertificate\Course 5\images/GitCommands.png)

##### Creating Github Repository

* One way is cloning an existing repository, the other is initializing our own
* Create our own project on our computer on our own machine
* Create an upstream repo on Github, copy repo's url
* Make local git repository: On the local machine, int he project directory, use **git init**. Not git starts tracking everything in the local repo.
* Now we need to start moving files into out local repo and connect local repo to the upstream remote by **git remote add origin <url>**.
* Note: The first time you push to a new repository, you may also need to use these git push options: **git push --set-upstream origin master**.

```
git init
git add README.txt
git commit -m "First commit. Adding README.txt file just to get started"
git remote add origin "https://github.com/ConnorBruce/murders.git"
git push    # you may need to add these arguments the first time: --set-upstream origin master
```
### Section 5: Advanced Unix

##### Arguments

* Arguments typically are defined using a dash or two dashes followed by a letter or a word
* **r**: recursive. For example **rm -r <directory-name>**: removes all files, subdirectories, files in subdirectories, subdirectories in subdirectories, etc...
* Combine arguments: **rm -rf directory-name**
* **ls -a**: Shows all files in the directories including hidden files (a for all)
* **ls -l**: Returns more information about the files (l for long)
* **ls -t**: Shows files in chronological order
* **ls -r**: Reverses the order of how the files are shown
* **ls -lart**: Shows more information for all the files in reverse order (combines the last 4)

##### Getting Help and Pipes

* Getting Help: Use **man + command name** to get help (e.g. **man ls**). Not that this is not available for Git Bash. For Git Bash, you can use **command --help** (e.g. **ls --help**)
* Pipes: Pipes the results of a command to the command after the pipe. Similar to the pipe %>% in R. For example, **man ls | less**.

##### Wild Cards

* star means any number of any combination of characters. Specifically, to list all html files **star.html**
* ? means any single character. For example, to erase all files in the form **file-001.html** with the numbers going from 1 to 999: **rm -file-???.html**

##### Environment Variables and Shells

* In Unix, variables are distinguished from other entities by adding a dollar sign in front. For example, the home directory is stores in **$HOME**
* See home directory: **echo $HOME**
* See them all: **env**
* See what shell is being used: **echo $SHELL** (most common shell is bash)

##### Executables, Permissions, and File Types

* In Unix, all programs are files. They are called executables. So **ls, mv, and git** are all files.
* To find where these program files are, use which. For example, **which git** would return /usr/bin/git.
* Type **ls /usr/bin** to see several executable files. There are other directories that hold program files (e.g Program Files directory on Windows).
* Type **echo $PATH** to see a list of directories separated by ":".
* Type the full path to run the user-created executables (e.g. **./my-ls**).
* Regular file **-**, directory **d**, executable **x**.
* This string also indicates the permission of the file: is it readable? writable? executable? Can other users on the system read the file? Can other users on the system edit the file? Can other users execute the file if the file is executable?

##### Other Imporant Commands

* Be aware of common commands and know what they do.
* **start filename**: Tries to figure out the right application of the filename and open it with that application.
* **nano**: A bare-bones text editor
* **ln**: Create a symbolic link.
* **tar**: Archive files and subdirectories of a directory into one file.
* **ssh**: connect to another computer
* **grep**: search for patterns in a file
* **awk/sed**: These are two very powerful commands that permit you to find specific string in files and change them.

## Course 6: Data Wrangling

### Section 1: Data Import

#### Importing Spreadsheets

* Many datasets are stored in spreadsheets. A spreadsheet is essentially a file version of a data frame with rows and columns.
* Spreadsheets have rows separated by returns and columns separated by a delimiter. The most common delimiters are commas, semicolons, white spaces, and tabs.
* Many spreadsheets are raw text files and can be read with any basic text editor. However, some formats are proprietary and cannont be read with a text editor, such as Excel Files.
* Most import functions assume that the first row of a spreadsheet is a header with column names. To know if the file has a header, it helps to look at a file with a text editor before trying to import it.

#### Paths and the Working Directory

* The working directory is where R looks for files and saves files by default
* See your workign directory with **getwd()**. Change your working directory with **setwd()**
* we suggest you create a firectory for each project and keep your raw data inside that directory.
* Use the **file.path()** function to generate a full path from a relative path and a file name. Use **file.path()** instead of **paste()** because **file.path()** is aware of your operating system and will use the correct slashes to navigate your machine.
* The **file.copy()** function copies a file to a new path.

Code to copy the murders data set to a different directory:
```
getwd() #Gets working directory
newpath = "C:/Users/conno/OneDrive/Documents/Data Science Certificate/HarvardxDataScienceCertificate/Course 6"
setwd(newpath) #Sets new working directory to newpath

path = system.file("extdata", package = "dslabs") #finds the dslabs file on computer
list.files(path)
filename = "murders.csv"
fullpath = file.path(path, filename)
fullpath #Gets the full path to the murders.csv file

file.copy(fullpath, getwd()) #copies the murders.csv file to current working directory

file.exists(filename) #verifies that the file was copied
```

#### The readr and readrxl Packages

* **readr** is the **tidyverse** library that includes functions for reading data stored in text file spreadsheets into R. Function in the package include: **read_csv()**, **read_tsv()**, **read_delim()**, and more. These differ by the delimited they use to split columns.
* The **readxl** package provides functions to read Excel formatted files.
* The **excel_sheets()** function gives the names of the sheets in the Excel file. These names are passed to the sheet argument for the readxl functions *read_excel()**, **read_xls()**, and **read_xlsx()**.
* The **read_lines()** function shows the first few lines of a file in R.
* If there is no header, when using a read function, add argument **col_names = FALSE**

Example using the packages:
```{r}
setwd("./Course 6")
#inspect the first 3 lines
read_lines("./data/murders.csv", n_max = 3)

#read file in csv format
dat = read_csv("./data/murders.csv")

#Examples
path = system.file("extdata", package = "dslabs")
files = list.files(path)
files

filename = "murders.csv"
filename1 = "life-expectancy-and-fertility-two-countries-example.csv"
filename2 = "fertility-two-countries-example.csv"
dat = read.csv(file.path(path, filename))
dat1 = read.csv(file.path(path, filename1))
dat2 = read.csv(file.path(path, filename2))
head(dat)
#head(dat1)
#head(dat2)
```

#### Importing Data using R-base Functions

* R-base import functions (**read.csv()**, **read.table()**, **read.delim()**) generate data frames rather than tibbles.

#### Downloading Files From the Internet

* The **read_csv()** function and other import functions can read a url directly.
* If you want to have a local copy of the file, you can use **download.file()**
* **tempdir()** creates a directory with a name that is very unlikely not to be unique.
* **tempfile()** creates a character string that is likely to be unique.

How to download (not executed because it's just an example)
```
url = "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
dat = read_csv(url)
#download.file(url, "murders.csv")
tempfile()
tmp_filename = tempfile()
#download.file(url, tmp_filename)
#dat = read.csv(tmp_filename)
#file.remove(tmp_filename)
```

### Section 2: Tidy Data

#### 2.1: Reshaping Data

##### Tidy Data

* In tidy data, each row represents an observation and each column represents a different variable.
* In wide data, each row includes several observations and one of the variables is stored in the header.

Example of tidy data vs. wide data:
```{r}
data(gapminder)

#create and inspect a tidy data frame
tidy_data = gapminder %>%
  filter(country %in% c("South Korea","Germany")) %>%
  select(country, year, fertility)
head(tidy_data)

#plotting tidy data is simple
tidy_data %>%
  ggplot(aes(year, fertility, color=country)) +
  geom_point()

#import and inspect example of original Gapminder data in wide format
path = system.file("extdata", package = "dslabs")
filename = file.path(path, "fertility-two-countries-example.csv")
wide_data = read_csv(filename)
select(wide_data,country,'1960':'1967')
```

##### Reshaping Data

* The **tidyr** package includes several functions that are useful for tidying data.
* The **gather()** function converts wide data into tidy data.
* The **spread()** function converts tidy data into wide data.

![Gather and Spread Info](C:\Users\conno\OneDrive\Documents\Data Science Certificate\HarvardxDataScienceCertificate\Course 6\images\gatherandspread.png)

Converting data from wide to tidy and vice-versa:
```{r}
#original wide data
path = system.file("extdata", package="dslabs")
filename = file.path(path, "fertility-two-countries-example.csv")
wide_data = read_csv(filename)

#tidy data from dslabs
data("gapminder")
tidy_data = gapminder %>%
  filter(country %in% c("South Korea","Germany")) %>%
  select(country, year, fertility)
head(tidy_data)

#gather wide data to make new tidy data
new_tidy_data = wide_data %>%
  gather(year, fertility, '1960':'2015')
head(new_tidy_data)

#gather all columns except country
new_tidy_data = wide_data %>%
  gather(year, fertility, -country)
head(new_tidy_data)

#gather treats column names as characters by default
class(tidy_data$year)
class(new_tidy_data$year)

#convert gathered column names to numeric
new_tidy_data = wide_data %>%
  gather(year, fertility, -country, convert = T)
class(new_tidy_data$year)

#spread tidy data to generate wide data
new_wide_data = new_tidy_data  %>% spread(year, fertility)
select(new_wide_data, country, '1960':'1967')
```

##### Separate and Unite

* The **separate()** function splits one column into two or more columns at a specified character that separates the variables.
* When there is an extra separaton in some of the entries, use **fill="right"** to pad missing values with NAs, or use **extra="merge"** to keep extra elements together.
* The **unite()** function combines two columns and adds a separating character.

Using separate and unite to tidy data:
```{r}
#import data
path = system.file("extdata", package ="dslabs")
filename = file.path(path, "life-expectancy-and-fertility-two-countries-example.csv")
raw_dat = read_csv(filename)
select(raw_dat,1:5)

#gather all columns except country
dat = raw_dat %>%gather(key, value, -country)
head(dat)
dat$key[1:5]

#separate on underscores
dat %>% separate(key, c("year","variable_name"),"_")
dat %>% separate(key, c("year","variable_name"))

#split on all underscores, pad empty cells with NA
dat %>% separate(key, c("year","first_variable_name","second_variable_name"),fill = "right")

#split on first underscore but keep life_expectancy merged
dat %>% separate(key, c("year","variable_name"),sep = "_",extra = "merge")

#separate then spread
dat %>% separate(key, c("year","variable_name"), sep = "_", extra = "merge") %>%
  spread(variable_name, value)

#separate then unite
dat %>% separate(key, c("year", "first_variable_name","second_variable_name"), fill = "right") %>%
  unite(variable_name, first_variable_name, second_variable_name, sep = "_")

#full code for tidying data
dat %>%
  separate(key, c("year", "first_variable_name", "second_variable_name"), fill = "right") %>%
  unite(variable_name, first_variable_name, second_variable_name, sep = "_") %>%
  spread(variable_name, value) %>%
  rename(fertility = fertility_NA)
```

#### 2.2: Combining Tables

* The **join()** function in the dplyr package combines two tables such that matching rows are together
* **left_join()** only keeps rows that have information in the first table
* **right_join()** only keeps rows that have information in the second table
* **inner_join()** only keeps rows that have information in both tables
* **full_join()** keeps all rows from both tables
* **semi_join()** keeps the part of the first table for which we have information in the second
* **anti_join()** keeps the elements of the first table for which there is no information in the second

Using the different join functions:
```{r}
#import US election results data
data(polls_us_election_2016)
head(results_us_election_2016)
identical(polls_us_election_2016, results_us_election_2016)

#join the murders table and us election results table
tab = left_join(murders, results_us_election_2016, by="state")
head(tab)

#plot electoral votes versus population
tab %>% ggplot(aes(population/10^6, electoral_votes,label=abb)) +
  geom_point() +
  geom_text_repel() +
  scale_x_continuous(trans = "log2") +
  scale_y_continuous(trans = "log2") +
  geom_smooth(method = "lm", se = F)
  
#make two smaller tables to demonstrate joins
tab1 = slice(murders, 1:6) %>% select(state, population)
tab1
tab2 = results_us_election_2016 %>% filter(state %in% c("Alabama","Alaska","Arizona","California","Connecticut","Delaware"))%>% arrange(by=state) %>% select(state, electoral_votes)
tab2

#experiment with different joins
left_join(tab1, tab2)
tab1 %>% left_join(tab2)
tab1 %>% right_join(tab2)
inner_join(tab1,tab2)
semi_join(tab1,tab2)
anti_join(tab1,tab2)
full_join(tab1,tab2)
```

##### Binding

* Unlike the join functions, the binding functions do not try to match by a variable, but rather just combine data sets
* **bind_cols()** binds two objects by making them columns in a tibble. The R-base functions **cbind()** binds columns but makes a data frame or matrix instead.
* **bind_rows()** is similar but binds rows instead of columns. The R-base function **rbind()** binds rows into a data frame or matrix.

Binding example:
```{r}
bind_cols(a = 1:3, b = 4:6)

tab1 = tab[,1:3]
tab2 = tab[,4:6]
tab3 = tab[,7:9]
new_tab = bind_cols(tab1,tab2,tab3)
head(new_tab)

tab1 = tab[1:2,]
tab2 = tab[3:4,]
bind_rows(tab1,tab2)
```

##### Set Operators

* By default, the set operators in R-base work on vectors. If tidyverse/dplyr are loaded, they also work on data frames
* You can take intersections of vectors using **intersect()**. This returns the elements common to both sets.
* You can take the union of vectors using **union()**. This returns the elements that are in either set.
* The set difference between a first and second argument can be obtained with **setdiff()**. Not that this function is not symmetric.
* The function **set_equal()** tells us if two sets are the same, regardless of the order of elements.

Example of the Set Operators:
```{r}
#intersect of data frames
intersect(1:10,6:15)
intersect(c("a","b","c"),c("b","c","d"))
tab1 = tab[1:5,]
tab2 = tab[3:7,]
intersect(tab1,tab2)

#perform a union of vectors or data frames
union(1:10,6:15)
union(c("a","b","c"),c("b","c","d"))
union(tab1,tab2)

#set difference of vectors or data frames
setdiff(1:10,6:15)
setdiff(c("a","b","c"),c("b","c","d"))
setdiff(tab1,tab2)

#setequal determines whether sets have the same elements regardless of order
setequal(1:5,1:6)
setequal(1:5,5:1)
setequal(tab1,tab2)
```

#### 2.3 Web Scraping

* Web scraping is extracting data from a website
* The **rvest** harvesting package includes functions to extract nodes of an HTML document: **html_nodes()** extracts all nodes of different types, and **html_node()** extracts the first node.
* **html_table()** converts an HTML table to a data frame.

Building the murders data set from the wiki page:
```{r}
library(readr)
library(readxl)
library(rvest)
url = "https://en.wikipedia.org/wiki/Murder_in_the_United_States_by_state"
h = read_html(url)
class(h)
h

tab = h %>% html_nodes("table")
tab = tab[[2]]

tab = tab %>% html_table()
class(tab)

tab = tab %>% setNames(c("state","population","total","murders","gun_murders","gun_ownership","total_rate","murder_rate","gun_murder_rate"))
head(tab)
```

* Most pages of a website use the same CSS file to build their sheets. Therefore, we can use this to our advantage to make it easier to extract information and data from these websites. 
* CSS leverages patterns to define elements such as font, color, size, and distance, also known as selectors. An example of a pattern is **table**.
* Knowing which selector to use is difficult. There is a program called SelectorGadget that can be used to determine which CSS selector you need to extract specific components of a webpage. There is a Chrome extension for this software.

As an example, here is extracting a Food Network recipe:
```{r}
#For the guac recipe, the selectors have already been determined
h = read_html("http://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")
recipe = h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time = h %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients = h %>% html_nodes(".o-Ingredients__a-Ingredient") %>% html_text()

#You can see how complex the selectors are
guacamole = list(recipe,prep_time,ingredients)
guacamole

#Since recipes from this website follow this general layout, we can write a function that extracts the information
get_recipe = function(url){
  h = read_html(url)
  recipe = h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
  prep_time = h %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
  ingredients = h %>% html_nodes(".o-Ingredients__a-Ingredient") %>% html_text()
  return(list(recipe = recipe, prep_time = prep_time,ingredients = ingredients))
  
}

#Testing function
get_recipe("http://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")
```

### Section 3: String Processing

#### String Processing Part 1

##### String Parsing

* The most common tasks in string parsing include:
+ extracting numbers from strings
+ removing unwanted characters from text
+ finding and replacing characters
+ extracting specific parts of strings
+ converting free form text to more uniform formats
+ splitting strings into multiple values
* The **stringr** package contains string processing function that follow a similar naming format and are compatible with the pipe.

Example of how data from websites may be in the wrong format (see commas in the population and total columns):
```{r}
url = "https://en.wikipedia.org/w/index.php?title=Gun_violence_in_the_United_States_by_state&direction=prev&oldid=810166167"
murders_raw = read_html(url) %>%
  html_nodes("table") %>%
  html_table() %>%
  .[[1]] %>%
  setNames(c("state","population","total","murder_rate"))

head(murders_raw)
class(murders_raw$population)
class(murders_raw$total)
```

##### Defining Strings/Escaping

* Define a string by surrounding text with either single or double quotes
* To include a single quote inside a string, use double quotes on the outside. To include a double quote inside a string, use single quotes on the outside.
* The **cat()** function displays a sting as it is represented inside R.
* To include a double quote inside of a string surrounded by double quotes, use the backslash to escape the double quote. Escaping a single qiote to include it inside of a string defined by single quotes.
* We will see additional uses of escape later.

Defining Strings Example:
```{r}
s = "Hello!" #double quotes defining a string
s = 'Hello!' #single quotes defining a string

s = '"Hello!' #double quote in single quot
cat(s)

s = "5'"
cat(s)

#To include both double and single quotes in a string, escape with \
s = '5\'10"'
cat(s)
s = "5'10\""
cat(s)
```

##### stringr Package

* The main types of string processing takes are detecting, locating, extracting, and replacing elements of a strings.
* The **stringr** package from the tidyverse includes a variety of string processing functions that begin with **str_** and tkae the string as the first argument, which makes the compatible with the pipe.
* Use the **str_detect()** function to determine whether a string contains a certain pattern.
* Use the **str_replace_all()** function to replace all instances of one pattern with another pattern. To remove a pattern, replace with empty string ("").
* The **parse_number()** function removes punctuation from strings and converts them to numeric.
* **mutate_at()** performs the same transformation on the specified column numbers.

Using these function to fix the murders dataset
```{r}
#detect whether there are any commas
commas = function(x) any(str_detect(x,","))
murders_raw %>% summarize_all(funs(commas))

#replace commas with the empty string and convert to numeric
test_1 = str_replace_all(murders_raw$population,",","")
test1 = as.numeric(test_1)
head(test_1)
class(test1)

#parse_number also removes commas and converts to numeric
test_2 = parse_number(murders_raw$population)
head(test_2)
class(test_2)

murders_new = murders_raw %>% mutate_at(2:3,parse_number)
murders_new %>% head()
```

#### String Processing Part 2

##### Case Study: Reported Heights

* In the raw heights data, many students did not report their height as the number of inches as requested. There are many entries with real height information but in the wrong format, which we can extract with string processing.
* When there are both text and numeric entries in a column, the column will be a character vector.
* To correct problematic entries, look for patterns that are shared across large numbers of entries, the define rules that identify those patterns and use these rules to write string processing tasks.
* Use **supressWarnings()** to hide warning messages for a function.

Case Study: Reported Heights code:
```{r}
#load raw heights data and inspect
data(reported_heights)
class(reported_heights$height)

#convert to numeric, inspect, count NAs
x = as.numeric(reported_heights$height)
head(x)
sum(is.na(x))

#keep only entries that result in NAs
reported_heights %>% mutate(new_height = as.numeric(height)) %>%
  filter(is.na(new_height)) %>%
  head(n=10)

#calculate cutoffs that calculate 99.99% of the human population
alpha = 1/10^6
qnorm(1-alpha/2,69.1,2.9)
qnorm(alpha/2,63.7,2.7)

#keep only entries that either results in NAs or are outside plausible range of heights
not_inches <- function(x, smallest = 50, tallest = 84){
  inches <- suppressWarnings(as.numeric(x))
  ind <- is.na(inches) | inches < smallest | inches > tallest
  ind
}

#number of problematic entries
problems = reported_heights %>%
  filter(not_inches(height)) %>%
  .$height
length(problems)

#10 examples of x'y or x'y" or x'y\"
pattern = "^\\d\\s*'\\s*\\d{1,2}\\.*\\d*'*\"*$"
str_subset(problems,pattern) %>% head(n=10) %>%cat

#10 examples of entries in cm rather than inches
ind <- which(between(suppressWarnings(as.numeric(problems))/2.54, 54, 81) )
ind <- ind[!is.na(ind)]
problems[ind] %>% head(n=10) %>% cat
```

##### Regular Expressions (Regex)

* A regular expression (regex) is a way to describe a specific patterns of characters of text. A set of rules has been designed to do this specifically and efficiently.
* **stringr** functions can take a regex as a pattern.
* **str_detect()** indicates whether a pattern is present in a string.
* The main difference between a regex and a regular string is that a regex can include special characters.
* The | symbol inside a regex means "or"
* use **\\d** to represent digits. The backslash is used to distinguish it from the character 'd'. In R, you must use two backslashes for digits in regular expressions; in some other languages, you will only use on backslash for regex special characters.
* **str_view()** highlights the first occurrence of a patterns, the the **str_view_all()** function highlights all occurrences of the patterns.

Regex Examples:
```{r}
#detect whether a comma is present
pattern = ","
str_detect(murders_raw$total,pattern)

#show the subset of strings including "cm"
str_subset(reported_heights$height, "cm")

#use the "or" symbol inside a regex (|)
yes = c("180 cm", "70 inches")
no = c("180","70''")
s = c(yes,no)
str_detect(s,"cm") | str_detect(s,"inches")
str_detect(s,"cm|inches")

#using \\d for digits
yes = c("5","6","5'10","5 feet", "4'11")
no = c("",".","Five","six")
s = c(yes,no)
pattern = "\\d"
str_detect(s,pattern)

#highlight occurrence of pattern
str_view(s,pattern)

#highlight all instances of pattern
str_view_all(s, pattern)
```

##### Character Classes, Anchors, and Quantifiers

* Define strings to test your regular expressions, including some elements that match and some that do not. This allows you to check for the two types of errors: failing to match, and matching incorrectly.
* Square brackets define character classes: groups of characters that count as a matching the pattern. You can use ranges to define character classes, such as [0-9] for digits and [a-zA-Z] for all letters.
* Anchors define patterns that must start or end at specific places. ^ and $ represent the beginning and end of the string respectively.
* Curly braces are quantifiers that state how many times a certain character can be repeated in the pattern. \\d{1,2} matches exactly 1 or 2 consecutive digits.

```{r}
#using \\d for digits
yes = c("5","6","5'10","5 feet", "4'11")
no = c("",".","Five","six")
s = c(yes,no)
pattern = "\\d"

#[56] means 5 or 6
str_view(s,"[56]")

#[4-7] means 4, 5, 6, or 7
yes = as.character(4:7)
no = as.character(1:3)
s = c(yes,no)
str_detect(s,"[4-7]")

#^ means start of a string, $ means end of string
pattern = "^\\d$"
yes = c("1","5","9")
no = c("12","123","1","a4","b")
s = c(yes,no)
str_view(s,pattern)

#curly braces define quantifiers: 1 or 2 digits
pattern = "^\\d{1,2}$"
yes = c("1","5","9","12")
no = c("123","a4","b")
str_view(c(yes,no),pattern)

#combining character class, anchors and quantifier
pattern = "^[4-5]'\\d{1,2}\"$"
yes <- c("5'7\"", "6'2\"",  "5'12\"")
no <- c("6,2\"", "6.2\"","I am 5'11\"", "3'2\"", "64")
str_detect(yes, pattern)
str_detect(no, pattern)
```


### Section 4: Dates, Times and Text Mining